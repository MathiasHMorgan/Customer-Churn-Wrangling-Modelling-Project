{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8270c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "695271ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_data_info(churnDataFrame: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Prints metadata of raw churn data set.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: None.\n",
    "    \"\"\"\n",
    "    print(f'Column Count: {len(churnDataFrame.columns)}')\n",
    "    print(f'Row Count: {len(churnDataFrame)}')\n",
    "    print(churnDataFrame.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b22fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_errors(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes rows where avg_frequency_login_days has an 'Error' value present.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn data set with error rows removed.\n",
    "    \"\"\"\n",
    "    rowCount = len(churnDataFrame)\n",
    "    errorCounts = len(churnDataFrame[churnDataFrame['avg_frequency_login_days'] == 'Error'])\n",
    "    churnDataFrame = churnDataFrame[churnDataFrame['avg_frequency_login_days'] != 'Error']\n",
    "    print(f'{errorCounts} Errors Rows Removed out of {rowCount}')\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20702181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes rows if there are duplicated \"security_no\" values in the data set.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn datas set with duplicate rows removed if present.\n",
    "    \"\"\"\n",
    "    originalSize = len(churnDataFrame)\n",
    "    churnDataFrame = churnDataFrame.drop_duplicates(subset='security_no', keep='first')\n",
    "    NewSize = len(churnDataFrame)\n",
    "    numDuplicates = originalSize - NewSize\n",
    "    print(f\"Number of duplicates removed: {numDuplicates}\")\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae289bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def churn_distrubtion(churnDataFrame: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Displays The percentage of customers that are and are not at risk of churn\n",
    "    if the \"churn_risk_score\" field is present. Some data sets in future may\n",
    "    not have this field present.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: None.\n",
    "    \"\"\"\n",
    "    cols = churnDataFrame.columns\n",
    "    if 'churn_risk_score' in cols:\n",
    "        print((churnDataFrame['churn_risk_score'].value_counts(\n",
    "            normalize=True) * 100).round(2).to_frame().T.rename(columns={1:'Percenrage At Risk',0:'Percentage Not At Risk'}))\n",
    "    else:\n",
    "        print('\"churn_risk_score\" filed not present in data set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92509174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_transform(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds extreme outlier values for known integer and float columns using IQR method\n",
    "    which are invalid records. Median value (excluding errenous values) is calculated\n",
    "    and imputed in place of invalid record. Results are rounded to two decimal places.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn data set with errenous errors imputed with the median and rounded.\n",
    "    \"\"\"\n",
    "\n",
    "    intColsTransform = ['days_since_last_login','age','avg_frequency_login_days']\n",
    "    floatColsTransform = ['avg_time_spent','avg_transaction_value','points_in_wallet']\n",
    "\n",
    "    for col in intColsTransform:\n",
    "        print(f'Processing Integer values in \"{col}\"...')\n",
    "        churnDataFrame[col] = abs(pd.to_numeric(churnDataFrame[col], errors='coerce')).round(0).astype('Int64')\n",
    "        Q1 = churnDataFrame[col].quantile(0.10)\n",
    "        Q3 = churnDataFrame[col].quantile(0.90)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 4 * IQR\n",
    "        upper_bound = Q3 + 4 * IQR\n",
    "        intOutliers = churnDataFrame[col][(churnDataFrame[col] < lower_bound) | (churnDataFrame[col] > upper_bound)]\n",
    "        if intOutliers.empty:\n",
    "            print(f'No far outliers found in {col}...')\n",
    "        else:\n",
    "            print(f'{len(intOutliers)} far outliers found in {col}: {sorted(set(intOutliers))}...')\n",
    "            medianVal = churnDataFrame[col].loc[~churnDataFrame.index.isin(intOutliers.index)].median()\n",
    "            churnDataFrame.loc[churnDataFrame.index.isin(intOutliers.index), col] = medianVal\n",
    "            print(f'The median valuse of {medianVal} imputed for all outliers')\n",
    "\n",
    "    for col in floatColsTransform:\n",
    "        print(f'Processing float values in \"{col}\"...')\n",
    "        churnDataFrame[col] = abs(pd.to_numeric(churnDataFrame[col], errors='coerce')).round(2).astype('float64')\n",
    "        Q1 = churnDataFrame[col].quantile(0.10)\n",
    "        Q3 = churnDataFrame[col].quantile(0.90)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 4 * IQR\n",
    "        upper_bound = Q3 + 4 * IQR\n",
    "        floatOutliers = churnDataFrame[col][(churnDataFrame[col] < lower_bound) | (churnDataFrame[col] > upper_bound)]\n",
    "        if floatOutliers.empty:\n",
    "            print(f'No far outliers found in {col}...')\n",
    "        else:\n",
    "            print(f'{len(floatOutliers)} far outliers found in {col}: {sorted(set(floatOutliers))}...')\n",
    "            medianVal = churnDataFrame[col].loc[~churnDataFrame.index.isin(floatOutliers.index)].median()\n",
    "            churnDataFrame.loc[churnDataFrame.index.isin(floatOutliers.index), col] = medianVal\n",
    "            print(f'The median valuse of {medianVal} imputed for all outliers')\n",
    "\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "024eaa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nulls(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identify fields containing null values and impute values based on field data type.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn data set with null values imputed based on filed type\n",
    "    \"\"\"\n",
    "    # Identify columns containing null values\n",
    "    print(\"Identify columns containing null records...\")\n",
    "    missingDataCols = list(churnDataFrame.isna().sum()[churnDataFrame.isna().sum() > 0].to_frame().T.columns)\n",
    "    for col in missingDataCols:\n",
    "        print(f\"{col} contains: {churnDataFrame[col].isna().sum()} null values\")\n",
    "\n",
    "    # Isolate a list of columns that are numeric, string and boolean based that contain missing values using list comprehensions\n",
    "    boolCols = ['used_special_discount','offer_application_preference', 'past_complaint']\n",
    "    stringCols = [col for col in list(churnDataFrame.select_dtypes(include='object').columns) if col in missingDataCols and col not in boolCols]\n",
    "    numericCols = [col for col in list(churnDataFrame.select_dtypes(include=['int64','float64']).columns) if col in missingDataCols]\n",
    "\n",
    "    print()\n",
    "    print(\"Processing null values in string columns...\")\n",
    "    for col in stringCols:\n",
    "        colNullCount = churnDataFrame[col].isna().sum()\n",
    "        print(f\"processing null {colNullCount} records in {col}...\")\n",
    "        churnDataFrame[col] = churnDataFrame[col].apply(lambda row: 'Unknown' if pd.isnull(row) else row)\n",
    "    print(\"Processing null values in numeric columns...\")\n",
    "    for col in numericCols:\n",
    "        colNullCount = churnDataFrame[col].isna().sum()\n",
    "        print(f\"processing null {colNullCount} records in {col}...\")\n",
    "        churnDataFrame[col] = (churnDataFrame[col].apply(lambda row: 0 if pd.isnull(row) else row)).round(2)\n",
    "    print(\"Processing null values in boolean columns...\")\n",
    "    for col in boolCols:\n",
    "        colNullCount = churnDataFrame[col].isna().sum()\n",
    "        print(f\"processing null {colNullCount} records in {col}...\")\n",
    "        churnDataFrame[col] = churnDataFrame[col].apply(lambda row: 'No' if pd.isnull(row) else row)\n",
    "    print()\n",
    "    print('Null records processed...')\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24138db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_transformation(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert \"joining_date\" field to a datetime data type allowing for date based splits\n",
    "    by year, month and day into separate fields.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn data set with date field split into separate fields.\n",
    "    \"\"\"\n",
    "    if 'joining_date' in churnDataFrame.columns:\n",
    "        churnDataFrame['joining_date'] = pd.to_datetime(churnDataFrame['joining_date'])\n",
    "        churnDataFrame['join_year'] = churnDataFrame['joining_date'].dt.year.astype('Int64')\n",
    "        churnDataFrame['join_month'] = churnDataFrame['joining_date'].dt.month.astype('Int64')\n",
    "        churnDataFrame['join_day'] = churnDataFrame['joining_date'].dt.day.astype('Int64')\n",
    "        churnDataFrame = churnDataFrame.drop(columns=['joining_date'])\n",
    "        print('joining_date field transformed...')\n",
    "        return churnDataFrame\n",
    "    else:\n",
    "        print('join_date field not in data...')\n",
    "        return churnDataFrame\n",
    "\n",
    "def time_transformation(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert \"last_visit_time\" field to a datetime data type in hours, minutes and seconds format to allow for\n",
    "    time based splits into separate fields.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn data set with time field split into separate fields.\n",
    "    \"\"\"\n",
    "    if 'last_visit_time' in churnDataFrame.columns:\n",
    "        churnDataFrame['last_visit_time'] = pd.to_datetime(churnDataFrame['last_visit_time'], format='%H:%M:%S')\n",
    "        churnDataFrame['last_visit_hour'] = churnDataFrame['last_visit_time'].dt.hour.astype('Int64')\n",
    "        churnDataFrame['last_visit_min'] = churnDataFrame['last_visit_time'].dt.minute.astype('Int64')\n",
    "        churnDataFrame['last_visit_sec'] = churnDataFrame['last_visit_time'].dt.second.astype('Int64')\n",
    "        churnDataFrame = churnDataFrame.drop(columns=['last_visit_time'])\n",
    "        print('last_visit_time field transformed...')\n",
    "        return churnDataFrame\n",
    "    else:\n",
    "        print('last_visit_time field not in data...')\n",
    "        return churnDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28c1adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_characters(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identifies columns containing special characters as values meaning they are\n",
    "    errenous and imputes other values. \"join_through_referral\" handled separatley.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn data set with errenous special characters removed and values imputed.\n",
    "    \"\"\"\n",
    "    specialChars = r'[!?#]'\n",
    "    cols_with_special = [\n",
    "        col for col in churnDataFrame.columns\n",
    "        if churnDataFrame[col].astype(str).str.contains(specialChars, regex=True).any()]\n",
    "    print(f'Columns containing special character values: {cols_with_special}')\n",
    "\n",
    "    for col in cols_with_special:\n",
    "        uniqueVals = list(set(churnDataFrame[col].to_list()))\n",
    "        SpecialCharCount = churnDataFrame[col].str.count(specialChars).sum()\n",
    "        if col in 'joined_through_referral':\n",
    "            churnDataFrame[col] = churnDataFrame.apply(\n",
    "                lambda row: 'No' if row['referral_id'] in ['xxxxxxxx'] and row['joined_through_referral'] in uniqueVals else 'Yes',\n",
    "                axis=1)\n",
    "        else:\n",
    "            churnDataFrame[col] = churnDataFrame[col].apply(lambda row: 'Unknown' if row in specialChars else row)\n",
    "        print(f'Processed {SpecialCharCount} records of special charcters in \"{col}\" column.....')\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccc9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_transform(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms feedback column into the following numeric classifications:\n",
    "    0: Negative\n",
    "    1: Positive\n",
    "    2: Netural\n",
    "    Results in this field are finite and can be transformed with harcoded mappings.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn data set with feedback values mapped to pre defined integers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Transforming feedback values to positive, negative or neutral integer mapping\n",
    "    feedBackMapping = {'Products always in Stock':1,\n",
    "    'User Friendly Website':1,\n",
    "    'Poor Customer Service':0,\n",
    "    'Poor Product Quality':0,\n",
    "    'Reasonable Price':1,\n",
    "    'Quality Customer Care':1,\n",
    "    'Too many ads':0,\n",
    "    'Poor Website':0,\n",
    "    'No reason specified':2}\n",
    "    # Display value mappings to log out put\n",
    "    for k,v in feedBackMapping.items():\n",
    "        print(f'Mapping value: \"{k}\", to integer value: {v}')\n",
    "\n",
    "    churnDataFrame['feedback'] = churnDataFrame['feedback'].str.strip()\n",
    "    churnDataFrame['feedback'] = churnDataFrame['feedback'].apply(lambda row: feedBackMapping[row] if row in feedBackMapping else row)\n",
    "    churnDataFrame['feedback'] = churnDataFrame['feedback'].astype('Int64')\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27ca5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform label encoding on all categorical fields, converting discrete values to integers\n",
    "    for tree based modelling downstream. Value mappings of categorical variables displayed\n",
    "    in log out put.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn data set with categorical variables conerted to integers\n",
    "    \"\"\"\n",
    "    categoricalCols = list(churnDataFrame.select_dtypes(include='object').columns)\n",
    "    for col in categoricalCols:\n",
    "        print(f'Label Encoding values in {col}...')\n",
    "        categoricalVals = list(set(churnDataFrame[col]))\n",
    "        labelEncoder = preprocessing.LabelEncoder()\n",
    "        churnDataFrame[col] = labelEncoder.fit_transform(churnDataFrame[col])\n",
    "        print(f'Categorical values mapped from {categoricalVals} to integer values: {list(set(churnDataFrame[col]))}')\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9df20aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove \"security_no\" and \"referral_id\" fields from the data set and re-order\n",
    "    the \"churn_risk_score\" field as the final colum reading left to right.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn data set with certain fields removed and columns re-ordered.\n",
    "    \"\"\"\n",
    "    churnDataFrame = churnDataFrame.drop(columns=['security_no','referral_id'])\n",
    "    print('Removed \"security_no\" and \"referral_id\" columns.....')\n",
    "    cols = [col for col in churnDataFrame.columns if col not in ['churn_risk_score']]\n",
    "    if 'churn_risk_score' in churnDataFrame.columns:\n",
    "        newCols = cols + ['churn_risk_score']\n",
    "        churnDataFrame = churnDataFrame.reindex(columns=newCols)\n",
    "        print('Re-Indexed \"churn_risk_score\" field')\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "042b8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_transform_pipeline() -> None:\n",
    "    \"\"\"\n",
    "    Function to execute the cleaning and transformation pipeline. This function\n",
    "    will call all specified functions sequentially to conduct the cleaning and\n",
    "    analysis process. Files will be taken based on file name and processed in a loop.\n",
    "    The cleaned CSV file will be written out to the \"clean\" folder.\n",
    "    Logging outputs will be printed below for visibility.\n",
    "    :returns: None\n",
    "    \"\"\"\n",
    "    # Extract a list of file names to be processed\n",
    "    csvFiles = [str(file) for file in Path('.').glob('*.csv')]\n",
    "    for fileName in csvFiles:\n",
    "        count = 0\n",
    "        data = pd.read_csv(fileName)\n",
    "\n",
    "        # Creating random ID to simulate a real data transformation pipeline in log output\n",
    "        randomId = uuid.uuid4()\n",
    "        stringID = str(randomId)\n",
    "        print(f\"Executing Data Trasnformation Pipeline...\")\n",
    "        print()\n",
    "        print(f\"Run ID: {stringID}\")\n",
    "        print()\n",
    "\n",
    "        print(\"----- Raw Churn Meta Data -----\",end='\\n')\n",
    "        meta_data_info(data)\n",
    "        print()\n",
    "\n",
    "        print(\"----- Removing Errenous Rows -----\",end='\\n')\n",
    "        data = remove_errors(data)\n",
    "        print()\n",
    "\n",
    "        print(\"----- Removing Duplicate Rows -----\",end='\\n')\n",
    "        data = remove_duplicates(data)\n",
    "        print()\n",
    "\n",
    "        print(\"----- Distribution (%) of Customers at risk of Churn -----\",end='\\n')\n",
    "        churn_distrubtion(data)\n",
    "        print()\n",
    "\n",
    "        print(\"----- Transform Numeric Values -----\",end='\\n')\n",
    "        data = numeric_transform(data)\n",
    "        print()\n",
    "\n",
    "        print(\"----- Impute Values for Null Records -----\",end='\\n')\n",
    "        data = process_nulls(data)\n",
    "        print()\n",
    "\n",
    "        print(\"----- Transform Date and Time Fields -----\",end='\\n')\n",
    "        data = date_transformation(data)\n",
    "        data = time_transformation(data)\n",
    "        print()\n",
    "\n",
    "        print(\"----- Impute Values for Special Characters -----\",end='\\n')\n",
    "        data = special_characters(data)\n",
    "        print()\n",
    "\n",
    "        print(\"----- Transform Feedback Column -----\",end='\\n')\n",
    "        data = feedback_transform(data)\n",
    "        print()\n",
    "\n",
    "        print(\"----- Removing Unnecessary Features -----\",end='\\n')\n",
    "        data = remove_features(data)\n",
    "        print()\n",
    "\n",
    "        print(\"----- Encode Categorical Variables -----\",end='\\n')\n",
    "        data = label_encode(data)\n",
    "        print()\n",
    "\n",
    "        print(f\"Data Transformation Run ID: {stringID} Complete...\")\n",
    "        fileName = f\"churn_clean_{count}.csv\"\n",
    "        cleanFolderPath = f\"clean/{fileName}\"\n",
    "        os.makedirs(\"clean\", exist_ok=True)\n",
    "        data.to_csv(cleanFolderPath, index=False)\n",
    "        print(f\"Transformed data set: {fileName} written to 'Clean' folder...\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3afdf742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Data Trasnformation Pipeline...\n",
      "\n",
      "Run ID: 67c08f0b-a190-415b-8cad-80d3506e1aaa\n",
      "\n",
      "----- Raw Churn Meta Data -----\n",
      "Column Count: 23\n",
      "Row Count: 36992\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36992 entries, 0 to 36991\n",
      "Data columns (total 23 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   age                           36992 non-null  int64  \n",
      " 1   gender                        36992 non-null  object \n",
      " 2   security_no                   36992 non-null  object \n",
      " 3   region_category               31564 non-null  object \n",
      " 4   membership_category           36992 non-null  object \n",
      " 5   joining_date                  36992 non-null  object \n",
      " 6   joined_through_referral       36992 non-null  object \n",
      " 7   referral_id                   36992 non-null  object \n",
      " 8   preferred_offer_types         36704 non-null  object \n",
      " 9   medium_of_operation           36992 non-null  object \n",
      " 10  internet_option               36992 non-null  object \n",
      " 11  last_visit_time               36992 non-null  object \n",
      " 12  days_since_last_login         36992 non-null  int64  \n",
      " 13  avg_time_spent                36992 non-null  float64\n",
      " 14  avg_transaction_value         36992 non-null  float64\n",
      " 15  avg_frequency_login_days      36992 non-null  object \n",
      " 16  points_in_wallet              33549 non-null  float64\n",
      " 17  used_special_discount         36992 non-null  object \n",
      " 18  offer_application_preference  36992 non-null  object \n",
      " 19  past_complaint                36992 non-null  object \n",
      " 20  complaint_status              36992 non-null  object \n",
      " 21  feedback                      36992 non-null  object \n",
      " 22  churn_risk_score              36992 non-null  int64  \n",
      "dtypes: float64(3), int64(3), object(17)\n",
      "memory usage: 6.5+ MB\n",
      "None\n",
      "\n",
      "----- Removing Errenous Rows -----\n",
      "3522 Errors Rows Removed out of 36992\n",
      "\n",
      "----- Removing Duplicate Rows -----\n",
      "Number of duplicates removed: 0\n",
      "\n",
      "----- Distribution (%) of Customers at risk of Churn -----\n",
      "churn_risk_score  Percenrage At Risk  Percentage Not At Risk\n",
      "proportion                     54.07                   45.93\n",
      "\n",
      "----- Transform Numeric Values -----\n",
      "Processing Integer values in \"days_since_last_login\"...\n",
      "1817 far outliers found in days_since_last_login: [999]...\n",
      "The median valuse of 13.0 imputed for all outliers\n",
      "Processing Integer values in \"age\"...\n",
      "No far outliers found in age...\n",
      "Processing Integer values in \"avg_frequency_login_days\"...\n",
      "No far outliers found in avg_frequency_login_days...\n",
      "Processing float values in \"avg_time_spent\"...\n",
      "No far outliers found in avg_time_spent...\n",
      "Processing float values in \"avg_transaction_value\"...\n",
      "No far outliers found in avg_transaction_value...\n",
      "Processing float values in \"points_in_wallet\"...\n",
      "No far outliers found in points_in_wallet...\n",
      "\n",
      "----- Impute Values for Null Records -----\n",
      "Identify columns containing null records...\n",
      "region_category contains: 4906 null values\n",
      "preferred_offer_types contains: 262 null values\n",
      "points_in_wallet contains: 3118 null values\n",
      "\n",
      "Processing null values in string columns...\n",
      "processing null 4906 records in region_category...\n",
      "processing null 262 records in preferred_offer_types...\n",
      "Processing null values in numeric columns...\n",
      "processing null 3118 records in points_in_wallet...\n",
      "Processing null values in boolean columns...\n",
      "processing null 0 records in used_special_discount...\n",
      "processing null 0 records in offer_application_preference...\n",
      "processing null 0 records in past_complaint...\n",
      "\n",
      "Null records processed...\n",
      "\n",
      "----- Transform Date and Time Fields -----\n",
      "joining_date field transformed...\n",
      "last_visit_time field transformed...\n",
      "\n",
      "----- Impute Values for Special Characters -----\n",
      "Columns containing special character values: ['joined_through_referral', 'medium_of_operation']\n",
      "Processed 4924 records of special charcters in \"joined_through_referral\" column.....\n",
      "Processed 4859 records of special charcters in \"medium_of_operation\" column.....\n",
      "\n",
      "----- Transform Feedback Column -----\n",
      "Mapping value: \"Products always in Stock\", to integer value: 1\n",
      "Mapping value: \"User Friendly Website\", to integer value: 1\n",
      "Mapping value: \"Poor Customer Service\", to integer value: 0\n",
      "Mapping value: \"Poor Product Quality\", to integer value: 0\n",
      "Mapping value: \"Reasonable Price\", to integer value: 1\n",
      "Mapping value: \"Quality Customer Care\", to integer value: 1\n",
      "Mapping value: \"Too many ads\", to integer value: 0\n",
      "Mapping value: \"Poor Website\", to integer value: 0\n",
      "Mapping value: \"No reason specified\", to integer value: 2\n",
      "\n",
      "----- Removing Unnecessary Features -----\n",
      "Removed \"security_no\" and \"referral_id\" columns.....\n",
      "Re-Indexed \"churn_risk_score\" field\n",
      "\n",
      "----- Encode Categorical Variables -----\n",
      "Label Encoding values in gender...\n",
      "Categorical values mapped from ['Unknown', 'M', 'F'] to integer values: [0, 1, 2]\n",
      "Label Encoding values in region_category...\n",
      "Categorical values mapped from ['Town', 'Village', 'Unknown', 'City'] to integer values: [0, 1, 2, 3]\n",
      "Label Encoding values in membership_category...\n",
      "Categorical values mapped from ['Premium Membership', 'Silver Membership', 'Gold Membership', 'No Membership', 'Basic Membership', 'Platinum Membership'] to integer values: [0, 1, 2, 3, 4, 5]\n",
      "Label Encoding values in joined_through_referral...\n",
      "Categorical values mapped from ['Yes', 'No'] to integer values: [0, 1]\n",
      "Label Encoding values in preferred_offer_types...\n",
      "Categorical values mapped from ['Gift Vouchers/Coupons', 'Unknown', 'Without Offers', 'Credit/Debit Card Offers'] to integer values: [0, 1, 2, 3]\n",
      "Label Encoding values in medium_of_operation...\n",
      "Categorical values mapped from ['Unknown', 'Smartphone', 'Both', 'Desktop'] to integer values: [0, 1, 2, 3]\n",
      "Label Encoding values in internet_option...\n",
      "Categorical values mapped from ['Wi-Fi', 'Mobile_Data', 'Fiber_Optic'] to integer values: [0, 1, 2]\n",
      "Label Encoding values in used_special_discount...\n",
      "Categorical values mapped from ['Yes', 'No'] to integer values: [0, 1]\n",
      "Label Encoding values in offer_application_preference...\n",
      "Categorical values mapped from ['Yes', 'No'] to integer values: [0, 1]\n",
      "Label Encoding values in past_complaint...\n",
      "Categorical values mapped from ['Yes', 'No'] to integer values: [0, 1]\n",
      "Label Encoding values in complaint_status...\n",
      "Categorical values mapped from ['Unsolved', 'Solved', 'No Information Available', 'Solved in Follow-up', 'Not Applicable'] to integer values: [0, 1, 2, 3, 4]\n",
      "\n",
      "Data Transformation Run ID: 67c08f0b-a190-415b-8cad-80d3506e1aaa Complete...\n",
      "Transformed data set: churn_clean_0.csv written to 'Clean' folder...\n"
     ]
    }
   ],
   "source": [
    "# Execute transformation pipeline\n",
    "execute_transform_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
