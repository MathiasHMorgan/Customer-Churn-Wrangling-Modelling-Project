{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "4bf9fdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.7.6 in /opt/conda/lib/python3.10/site-packages (1.7.6)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xgboost==1.7.6) (1.26.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost==1.7.6) (1.11.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost==1.7.6\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.functions import col, abs\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import when\n",
    "import uuid\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "92d5c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ChurnXGBoost\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "4de1d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_churn_data(fileName: str) -> DataFrame:\n",
    "    try:\n",
    "        churnDataframe = spark.read.csv(fileName, header=True, inferSchema=True)\n",
    "        print(\"Churn data loaded...\")\n",
    "        return churnDataframe\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "254013f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_churn_metadata(churnDataFrame) -> None:\n",
    "    tempPandasDf = churnDataFrame.toPandas() # Localised spark df conversion to pandas\n",
    "    tempPandasDf.info() # Display metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "f0a958c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_feature_target_cols(churnDataFrame) -> [list,str]:\n",
    "    featureCols = churnDataFrame.columns[:-1]\n",
    "    targetCol = churnDataFrame.columns[-1]\n",
    "    print(f\"All Feature Columns: {featureCols}\")\n",
    "    print()\n",
    "    print(f\"Target Column: {targetCol}\")\n",
    "    return featureCols, targetCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "e63d0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(churnDataFrame: DataFrame, featureCols: list,\n",
    "                         targetCol: str, moderateVal: float, strongVal: float) -> None:\n",
    "    print(\"Begin correlation analysis of all features...\\n\")\n",
    "\n",
    "    corrVals = []\n",
    "    for feature in featureCols:\n",
    "        corr = churnDataFrame.stat.corr(feature, targetCol)\n",
    "        corrVals.append((feature, corr))\n",
    "\n",
    "    corrDf = spark.createDataFrame(corrVals, [\"feature\", \"pearson_correlation\"]) \\\n",
    "                  .orderBy(abs(col(\"pearson_correlation\")).desc())\n",
    "\n",
    "    corrDf = corrDf.withColumn(\"feature_strength\",\n",
    "         when(abs(col(\"pearson_correlation\")) > strongVal, \"Strong\")\n",
    "        .when(abs(col(\"pearson_correlation\")) > moderateVal, \"Moderate\")\n",
    "        .otherwise(\"Weak\"))\n",
    "    print(\"All Correlation Scores:\")\n",
    "    corrDf.show(n=corrDf.count(), truncate=False)\n",
    "    print()\n",
    "    print(\"Features displaying the strongest predictive signal:\")\n",
    "    filteredCorrDf = corrDf.filter(col(\"feature_strength\").isin(\"Strong\", \"Moderate\"))\n",
    "    filteredCorrDf.orderBy(abs(col(\"pearson_correlation\")).desc()).show(truncate=False)\n",
    "    strongCorrFeatures = filteredCorrDf.select(\"feature\").rdd.flatMap(lambda x: x).collect()\n",
    "    return strongCorrFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "40a457f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_assemble(churnDataframe, featureCols, trainSplit:float, testSplit:float) -> [DataFrame, DataFrame]:\n",
    "    print(\"Assembling features...\")\n",
    "    print(\"Combining all features into single vector...\")\n",
    "    assembler_all = VectorAssembler(inputCols = featureCols,\n",
    "                                    outputCol=\"features\") # Combine all features into a vector\n",
    "    pipeline = Pipeline(stages=[assembler_all])\n",
    "    vectorDf = pipeline.fit(churnDataframe).transform(churnDataframe)\n",
    "    print(\"Splitting data...\")\n",
    "    print(f\"{trainSplit*100}/{testSplit*100} Test Train Split...\")\n",
    "    trainDf, testDf = vectorDf.randomSplit([trainSplit, testSplit], seed=42)\n",
    "    return trainDf, testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "2fe766d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(trainDf, testDf, targetCol) -> [DataFrame, str]:\n",
    "    xgbClassifier = SparkXGBClassifier(\n",
    "            features_col=\"features\",\n",
    "            label_col=targetCol,\n",
    "            prediction_col=\"prediction\",\n",
    "            num_workers=spark.sparkContext.defaultParallelism )\n",
    "    xgbModel = xgbClassifier.fit(trainDf)\n",
    "    predictions = xgbModel.transform(testDf)\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=targetCol,\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"f1\")\n",
    "    f1Score = evaluator.evaluate(predictions)\n",
    "    print(f\"Baseline model F1 Score - All Features: {f1Score}\")\n",
    "    return xgbModel, f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "6f22c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_information_gain(model, featureCols) -> [str]:\n",
    "    importances = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    print(\"All Feature Importances by Information Gain:\")\n",
    "\n",
    "    featureMap = {f\"f{i}\": name for i, name in enumerate(featureCols)}\n",
    "    sortedImportances = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    featureDict = {}\n",
    "    for feature, score in sortedImportances:\n",
    "        feature = featureMap.get(feature, feature)\n",
    "        featureDict[feature] = score\n",
    "    scoreList = [[featureMap.get(feat, feat), score] for feat, score in sortedImportances]\n",
    "    allFeatureGainDf = spark.createDataFrame(scoreList, [\"feature\", \"gain\"])\n",
    "    allFeatureGainDf = allFeatureGainDf.withColumn(\"gain_strength\",\n",
    "         when(abs(col(\"gain\")) > 10, \"Strong\")\n",
    "        .when(abs(col(\"gain\")) > 3, \"Moderate\")\n",
    "        .otherwise(\"Weak\"))\n",
    "    allFeatureGainDf.show(n=allFeatureGainDf.count(), truncate=False)\n",
    "    print(\"Features with stronger information gain scores...\")\n",
    "    strongGainDf = allFeatureGainDf.filter(col(\"gain_strength\").isin(\"Strong\", \"Moderate\"))\n",
    "    strongGainDf.orderBy(abs(col(\"gain_strength\")).desc()).show(truncate=False)\n",
    "    strongGainFeatures = strongGainDf.select(\"feature\").rdd.flatMap(lambda x: x).collect()\n",
    "    return strongGainFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "d8b8a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strong_features(corrFeatures: list, gainFeatures: list) -> [str]:\n",
    "        strongFeatures = list(set(corrFeatures + gainFeatures))\n",
    "        print(f\"Strong feature list: {strongFeatures}\")\n",
    "        return strongFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "f6d4bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_ml_pipeline():\n",
    "\n",
    "    print(\"Spark session created: ChurnXGBoost \")\n",
    "    print()\n",
    "\n",
    "    randomId = uuid.uuid4()\n",
    "    stringID = str(randomId)\n",
    "    print(f\"Executing Machine Learning Pipeline...\")\n",
    "    print() # Creates a gap for for clean logging output\n",
    "    print(f\"Run ID: {stringID}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"***** GET DATA *****\", end=\"\\n\")\n",
    "    print()\n",
    "\n",
    "    print(\"----- Load Cleaned Data -----\", end=\"\\n\")\n",
    "    churnData = get_churn_data(\"churn_clean.csv\")\n",
    "    print()\n",
    "    \n",
    "    print(\"----- Cleaned Churn Metadata -----\", end=\"\\n\")\n",
    "    get_churn_metadata(churnData)\n",
    "    print()\n",
    "    \n",
    "    print(\"***** TRAIN AND SCORE BASELINE MODEL *****\", end=\"\\n\")\n",
    "    print()\n",
    "\n",
    "    print(\"----- Baseline Model: Separate Feature and Target Column/s -----\", end=\"\\n\")\n",
    "    featureCols, targetCol = separate_feature_target_cols(churnData)\n",
    "    print()\n",
    "\n",
    "    print(\"----- Baseline Model: Correlation Analysis -----\", end=\"\\n\")\n",
    "    strongCorrFeatures = correlation_analysis(churnData, featureCols, targetCol, 0.2, 0.4)\n",
    "    print()\n",
    "\n",
    "    print(\"----- Baseline Model: Assemble Features -----\", end=\"\\n\")\n",
    "    trainSetDf, testSetDf = feature_assemble(churnData, featureCols, trainSplit=0.8, testSplit=0.2)\n",
    "    print()\n",
    "\n",
    "    print(\"----- Baseline Model: Create and Score -----\",end=\"\\n\")\n",
    "    xgbModel, BaselineF1score = baseline_model(trainSetDf, testSetDf, targetCol)\n",
    "    print()\n",
    "    \n",
    "    print(\"----- Baseline Model: Feature Information Gain -----\",end=\"\\n\")\n",
    "    strongGainFeatures = feature_information_gain(xgbModel, featureCols)\n",
    "    print()\n",
    "    \n",
    "    print(\"***** TRAIN AND SCORE OPTIMISED MODEL *****\",end=\"\\n\")\n",
    "    print()\n",
    "    \n",
    "    print(\"----- Optimised Model: Get strongest features -----\",end=\"\\n\")\n",
    "    strongFeatures = get_strong_features(strongCorrFeatures,strongGainFeatures)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "085b9b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created: ChurnXGBoost \n",
      "\n",
      "Executing Machine Learning Pipeline...\n",
      "\n",
      "Run ID: b27ab952-d314-49d1-9b00-476d837be206\n",
      "\n",
      "***** GET DATA *****\n",
      "\n",
      "----- Load Cleaned Data -----\n",
      "Churn data loaded...\n",
      "\n",
      "----- Cleaned Churn Metadata -----\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 25 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   age                           20000 non-null  int32  \n",
      " 1   gender                        20000 non-null  int32  \n",
      " 2   region_category               20000 non-null  int32  \n",
      " 3   membership_category           20000 non-null  int32  \n",
      " 4   joined_through_referral       20000 non-null  int32  \n",
      " 5   preferred_offer_types         20000 non-null  int32  \n",
      " 6   medium_of_operation           20000 non-null  int32  \n",
      " 7   internet_option               20000 non-null  int32  \n",
      " 8   days_since_last_login         20000 non-null  int32  \n",
      " 9   avg_time_spent                20000 non-null  float64\n",
      " 10  avg_transaction_value         20000 non-null  float64\n",
      " 11  avg_frequency_login_days      20000 non-null  int32  \n",
      " 12  points_in_wallet              20000 non-null  float64\n",
      " 13  used_special_discount         20000 non-null  int32  \n",
      " 14  offer_application_preference  20000 non-null  int32  \n",
      " 15  past_complaint                20000 non-null  int32  \n",
      " 16  complaint_status              20000 non-null  int32  \n",
      " 17  feedback                      20000 non-null  int32  \n",
      " 18  join_year                     20000 non-null  int32  \n",
      " 19  join_month                    20000 non-null  int32  \n",
      " 20  join_day                      20000 non-null  int32  \n",
      " 21  last_visit_hour               20000 non-null  int32  \n",
      " 22  last_visit_min                20000 non-null  int32  \n",
      " 23  last_visit_sec                20000 non-null  int32  \n",
      " 24  churn_risk_score              20000 non-null  int32  \n",
      "dtypes: float64(3), int32(22)\n",
      "memory usage: 2.1 MB\n",
      "\n",
      "***** TRAIN AND SCORE BASELINE MODEL *****\n",
      "\n",
      "----- Baseline Model: Separate Feature and Target Column/s -----\n",
      "All Feature Columns: ['age', 'gender', 'region_category', 'membership_category', 'joined_through_referral', 'preferred_offer_types', 'medium_of_operation', 'internet_option', 'days_since_last_login', 'avg_time_spent', 'avg_transaction_value', 'avg_frequency_login_days', 'points_in_wallet', 'used_special_discount', 'offer_application_preference', 'past_complaint', 'complaint_status', 'feedback', 'join_year', 'join_month', 'join_day', 'last_visit_hour', 'last_visit_min', 'last_visit_sec']\n",
      "\n",
      "Target Column: churn_risk_score\n",
      "\n",
      "----- Baseline Model: Correlation Analysis -----\n",
      "Begin correlation analysis of all features...\n",
      "\n",
      "All Correlation Scores:\n",
      "+----------------------------+----------------------+----------------+\n",
      "|feature                     |pearson_correlation   |feature_strength|\n",
      "+----------------------------+----------------------+----------------+\n",
      "|membership_category         |-0.46364897254549664  |Strong          |\n",
      "|avg_transaction_value       |-0.21665332101983684  |Moderate        |\n",
      "|points_in_wallet            |-0.207422685724642    |Moderate        |\n",
      "|avg_frequency_login_days    |0.14265642085882052   |Weak            |\n",
      "|feedback                    |-0.12766442395345956  |Weak            |\n",
      "|preferred_offer_types       |0.04214231250899457   |Weak            |\n",
      "|joined_through_referral     |0.029778826416374117  |Weak            |\n",
      "|days_since_last_login       |0.023986893637957075  |Weak            |\n",
      "|past_complaint              |0.020529307755128675  |Weak            |\n",
      "|used_special_discount       |-0.01642696016799114  |Weak            |\n",
      "|offer_application_preference|-0.01603001881306162  |Weak            |\n",
      "|join_day                    |0.009984958262612123  |Weak            |\n",
      "|avg_time_spent              |-0.009502113982954943 |Weak            |\n",
      "|complaint_status            |0.009221781356913678  |Weak            |\n",
      "|join_month                  |0.008882262339822906  |Weak            |\n",
      "|medium_of_operation         |0.008651999786633533  |Weak            |\n",
      "|age                         |0.007526928941161889  |Weak            |\n",
      "|region_category             |-0.0069399457712400175|Weak            |\n",
      "|internet_option             |-0.006439232662519857 |Weak            |\n",
      "|last_visit_hour             |-0.00565745485573185  |Weak            |\n",
      "|last_visit_sec              |0.0054779529628769105 |Weak            |\n",
      "|gender                      |-0.004130582425128453 |Weak            |\n",
      "|last_visit_min              |0.003876484162647322  |Weak            |\n",
      "|join_year                   |0.0027310797966685505 |Weak            |\n",
      "+----------------------------+----------------------+----------------+\n",
      "\n",
      "\n",
      "Features displaying the strongest predictive signal:\n",
      "+---------------------+--------------------+----------------+\n",
      "|feature              |pearson_correlation |feature_strength|\n",
      "+---------------------+--------------------+----------------+\n",
      "|membership_category  |-0.46364897254549664|Strong          |\n",
      "|avg_transaction_value|-0.21665332101983684|Moderate        |\n",
      "|points_in_wallet     |-0.207422685724642  |Moderate        |\n",
      "+---------------------+--------------------+----------------+\n",
      "\n",
      "\n",
      "----- Baseline Model: Assemble Features -----\n",
      "Assembling features...\n",
      "Combining all features into single vector...\n",
      "Splitting data...\n",
      "80.0/20.0 Test Train Split...\n",
      "\n",
      "----- Baseline Model: Create and Score -----\n",
      "Baseline model F1 Score - All Features: 0.9367794506345011\n",
      "\n",
      "----- Baseline Model: Feature Information Gain -----\n",
      "All Feature Importances by Information Gain:\n",
      "+----------------------------+------------------+-------------+\n",
      "|feature                     |gain              |gain_strength|\n",
      "+----------------------------+------------------+-------------+\n",
      "|membership_category         |48.6011962890625  |Strong       |\n",
      "|points_in_wallet            |45.45088577270508 |Strong       |\n",
      "|feedback                    |9.6278657913208   |Moderate     |\n",
      "|avg_transaction_value       |4.120265960693359 |Moderate     |\n",
      "|internet_option             |2.3006558418273926|Weak         |\n",
      "|avg_frequency_login_days    |2.2145590782165527|Weak         |\n",
      "|preferred_offer_types       |2.1098990440368652|Weak         |\n",
      "|region_category             |2.072117805480957 |Weak         |\n",
      "|last_visit_min              |2.067615270614624 |Weak         |\n",
      "|last_visit_hour             |2.058248996734619 |Weak         |\n",
      "|last_visit_sec              |2.0136029720306396|Weak         |\n",
      "|medium_of_operation         |2.0098652839660645|Weak         |\n",
      "|join_day                    |2.001390218734741 |Weak         |\n",
      "|avg_time_spent              |1.9984875917434692|Weak         |\n",
      "|age                         |1.9868335723876953|Weak         |\n",
      "|joined_through_referral     |1.9840564727783203|Weak         |\n",
      "|join_year                   |1.9566161632537842|Weak         |\n",
      "|days_since_last_login       |1.9301865100860596|Weak         |\n",
      "|complaint_status            |1.8957364559173584|Weak         |\n",
      "|join_month                  |1.725785255432129 |Weak         |\n",
      "|offer_application_preference|1.7144598960876465|Weak         |\n",
      "|used_special_discount       |1.5760178565979004|Weak         |\n",
      "|gender                      |1.5094637870788574|Weak         |\n",
      "|past_complaint              |1.4234338998794556|Weak         |\n",
      "+----------------------------+------------------+-------------+\n",
      "\n",
      "Features with stronger information gain scores...\n",
      "+---------------------+-----------------+-------------+\n",
      "|feature              |gain             |gain_strength|\n",
      "+---------------------+-----------------+-------------+\n",
      "|membership_category  |48.6011962890625 |Strong       |\n",
      "|feedback             |9.6278657913208  |Moderate     |\n",
      "|points_in_wallet     |45.45088577270508|Strong       |\n",
      "|avg_transaction_value|4.120265960693359|Moderate     |\n",
      "+---------------------+-----------------+-------------+\n",
      "\n",
      "\n",
      "***** TRAIN AND SCORE OPTIMISED MODEL *****\n",
      "\n",
      "----- Optimised Model: Get strongest features -----\n",
      "Strong feature list: ['membership_category', 'points_in_wallet', 'avg_transaction_value', 'feedback']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "execute_ml_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "10f5d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ─────────────────────────────────────────────────────────────\n",
    "#            PHASE 1: BASELINE TRAINING WITH ALL FEATURES\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# def load_data(file_path):\n",
    "#     # Read CSV, infer schema, return Spark DataFrame\n",
    "#     return df\n",
    "\n",
    "# def separate_features_and_target(df):\n",
    "#     features = df.columns[:-1]\n",
    "#     target = df.columns[-1]\n",
    "#     return features, target\n",
    "\n",
    "# def run_correlation_analysis(df, features, target):\n",
    "#     # Compute Pearson correlation between each feature and target\n",
    "#     # Optionally label as 'Strong', 'Moderate', 'Weak'\n",
    "#     return correlation_df\n",
    "\n",
    "# def assemble_features(df, featureCols):\n",
    "#     # Use VectorAssembler to create 'features' column\n",
    "#     return assembled_df\n",
    "\n",
    "# def train_baseline_model(train_df, target_col):\n",
    "#     # Fit XGBoost on full feature set\n",
    "#     return model\n",
    "\n",
    "# def evaluate_model(model, test_df, target_col):\n",
    "#     # Get predictions, compute F1 score, return evaluation\n",
    "#     return f1_score, predictions\n",
    "\n",
    "# def log_baseline_results(f1_score, feature_importance, correlation_df):\n",
    "#     # Store results for comparison\n",
    "#     pass\n",
    "\n",
    "# # ─────────────────────────────────────────────────────────────\n",
    "# #          PHASE 2: FEATURE + PARAMETER OPTIMIZATION\n",
    "# # ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# def filter_features_by_correlation(correlation_df, threshold=0.2):\n",
    "#     # Return only features with abs(corr) > threshold\n",
    "#     return selected_features\n",
    "\n",
    "# def run_kfold_cv(train_df, estimator, param_grid, evaluator, k=5):\n",
    "#     # Setup CrossValidator with K folds and param grid\n",
    "#     # Return best model and best params\n",
    "#     return best_model, best_params\n",
    "\n",
    "# def log_optimized_results(f1_score, best_params, feature_set):\n",
    "#     # Save to tracking system or file\n",
    "#     pass\n",
    "\n",
    "# # ─────────────────────────────────────────────────────────────\n",
    "# #                  PHASE 3: PIPELINE DRIVER\n",
    "# # ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# def full_pipeline(file_path):\n",
    "#     df = load_data(file_path)\n",
    "#     featureCols, targetCol = separate_features_and_target(df)\n",
    "\n",
    "#     # PHASE 1: BASELINE\n",
    "#     correlation_df = run_correlation_analysis(df, featureCols, targetCol)\n",
    "#     df_all = assemble_features(df, featureCols)\n",
    "#     train_df, test_df = split_data(df_all)\n",
    "\n",
    "#     baseline_model = train_baseline_model(train_df, targetCol)\n",
    "#     f1_base, _ = evaluate_model(baseline_model, test_df, targetCol)\n",
    "#     log_baseline_results(f1_base, get_importance(baseline_model), correlation_df)\n",
    "\n",
    "#     # PHASE 2: OPTIMIZATION\n",
    "#     good_feats = filter_features_by_correlation(correlation_df, threshold=0.2)\n",
    "#     df_opt = assemble_features(df, good_feats)\n",
    "#     train_opt, test_opt = split_data(df_opt)\n",
    "\n",
    "#     best_model, best_params = run_kfold_cv(train_opt, xgb_estimator, param_grid, evaluator)\n",
    "#     f1_opt, _ = evaluate_model(best_model, test_opt, targetCol)\n",
    "#     log_optimized_results(f1_opt, best_params, good_feats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
