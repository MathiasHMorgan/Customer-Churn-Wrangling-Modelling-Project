{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "4bf9fdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.7.6 in /opt/conda/lib/python3.10/site-packages (1.7.6)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xgboost==1.7.6) (1.26.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost==1.7.6) (1.11.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost==1.7.6\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.functions import col, abs\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier, GBTClassificationModel, ClassificationModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import lit\n",
    "from pathlib import Path\n",
    "from pyspark import StorageLevel\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "import uuid\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import timedelta\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "0ab2b5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.shuffle.partitions -> 24\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ChurnXGBoost\").master(\"local[*]\").getOrCreate()\n",
    "cores = spark.sparkContext.defaultParallelism \n",
    "target_shuffle_partitions = cores * 2 # set custom partitions for shuffling based on available cores\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", target_shuffle_partitions) # will use above coun during shuffle operations\n",
    "print(\"spark.sql.shuffle.partitions ->\", spark.conf.get(\"spark.sql.shuffle.partitions\")) # displays partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "fdd34240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_clean_churn_files() -> DataFrame | None:\n",
    "    \"\"\"\n",
    "    Combines all 'churn_clean*.csv' files in current directory into one DataFrame for processing.\n",
    "    Returns None if no matching files found. First file determines schema.\n",
    "    \"\"\"\n",
    "    csvFiles = [str(file) for file in Path(\".\").glob(\"churn_clean*.csv\")]\n",
    "    print(f\"All available cleaned churn csv files: {csvFiles}\")\n",
    "\n",
    "    if not csvFiles:\n",
    "        print(\"No churn_cleaned CSV files found...\")\n",
    "        return None\n",
    "    count = 0\n",
    "    for fileName in csvFiles:\n",
    "        if count == 0:\n",
    "            churnCombinedDf = spark.read.csv(fileName, header=True, inferSchema=True)\n",
    "            count += 1\n",
    "        else:\n",
    "            churnCombinedDf = churnCombinedDf.unionByName(churnCombinedDf)\n",
    "    return churnCombinedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "254013f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_churn_metadata(churnDataFrame: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Prints basic DataFrame metadata including column dtypes and non-null counts for user visibility.\n",
    "    Temporarily converts Spark DataFrame to pandas for clean meta data outut.\n",
    "    \"\"\"\n",
    "    tempPandasDf = churnDataFrame.toPandas() \n",
    "    tempPandasDf.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "319b2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe_by_label(churnDataFrame: DataFrame) -> tuple[DataFrame, DataFrame, bool]:\n",
    "    \"\"\"\n",
    "    Splits DataFrame into labeled and unlabeled sets based on if churn_risk_score contains empty rows.\n",
    "    If no unlabeled data exists, reserves 1% of labeled data for demonstrative prediction purposes.\n",
    "    Args:\n",
    "        churnDataFrame: Input DataFrame containing cleaned churn data\n",
    "    Returns:\n",
    "        churnlabeled: Rows with churn_risk_score\n",
    "        churnunlabeled: Rows without churn_risk_score or reserved data\n",
    "        reservedFlag: True if data was reserved\n",
    "    \"\"\"\n",
    "    churnlabeled = churnDataFrame.filter(col('churn_risk_score').isNotNull())\n",
    "    churnunlabeled = churnDataFrame.filter(col('churn_risk_score').isNull())\n",
    "    reserve = 0.01\n",
    "    reservedFlag = False\n",
    "    if churnunlabeled.count() == 0:\n",
    "        reservedFlag = True\n",
    "        print('no empty churn_risk_score rows...')\n",
    "        print(f\"Reserving {int(1)}% of labeled data for demo predictions...\")\n",
    "        trainDf, reservedDf = churnlabeled.randomSplit([1 - reserve, reserve], seed=42)\n",
    "        churnlabeled = trainDf\n",
    "        churnunlabeled = reservedDf\n",
    "        print(f\"Reserve dataframe with {churnunlabeled.count()} rows\")\n",
    "        churnunlabeled = churnunlabeled.withColumnRenamed(\"churn_risk_score\", \"actual_churn_risk_score\")\n",
    "        churnunlabeled = churnunlabeled.withColumn(\"predicted_churn_risk_score\", lit(None))\n",
    "    return churnlabeled, churnunlabeled, reservedFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "f0a958c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_feature_target_cols(churnDataFrame: DataFrame) -> tuple[list,str]:\n",
    "    \"\"\"\n",
    "    Separates the input DataFrame columns into features and target variable.\n",
    "    Uses all columns except the last as features, and the last column as target.\n",
    "    Args:\n",
    "        churnDataFrame: Input DataFrame containing cleaned churn data\n",
    "    Returns:\n",
    "        featureCols: List of all feature column names\n",
    "        targetCol: Name of the target column to predict\n",
    "    \"\"\"\n",
    "    featureCols = churnDataFrame.columns[:-1]\n",
    "    targetCol = churnDataFrame.columns[-1]\n",
    "    print(f\"All Feature Columns: {featureCols}\")\n",
    "    print(f\"Target Column: {targetCol}\")\n",
    "    return featureCols, targetCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "e63d0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(churnDataFrame: DataFrame, featureCols: list,\n",
    "                         targetCol: str, moderateVal: float, strongVal: float) -> list:\n",
    "    \"\"\"\n",
    "    Performs correlation analysis between features and target variable.\n",
    "    Identifies features with strong, moderate and weak correlation based on threshold values.\n",
    "    Args:\n",
    "        churnDataFrame: Input DataFrame containing features and target\n",
    "        featureCols: List of feature column names to analyze\n",
    "        targetCol: Name of the target column\n",
    "        moderateVal: Threshold for moderate correlation\n",
    "        strongVal: Threshold for strong correlation\n",
    "    Returns:\n",
    "        list: Features showing strong/moderate correlation with target\n",
    "    \"\"\"\n",
    "    print(\"Begin correlation analysis of all features...\\n\")\n",
    "\n",
    "    corrVals = []\n",
    "    for feature in featureCols:\n",
    "        corr = churnDataFrame.stat.corr(feature, targetCol)\n",
    "        corrVals.append((feature, corr))\n",
    "\n",
    "    corrDf = spark.createDataFrame(corrVals, [\"feature\", \"pearson_correlation\"]) \\\n",
    "                  .orderBy(abs(col(\"pearson_correlation\")).desc())\n",
    "    corrDf = corrDf.withColumn(\"feature_strength\",\n",
    "         when(abs(col(\"pearson_correlation\")) > strongVal, \"Strong\")\n",
    "        .when(abs(col(\"pearson_correlation\")) > moderateVal, \"Moderate\")\n",
    "        .otherwise(\"Weak\"))\n",
    "    \n",
    "    print(\"All Correlation Scores:\")\n",
    "    corrDf.show(n=corrDf.count(), truncate=False)\n",
    "    print()\n",
    "    print(\"Features displaying the strongest predictive signal:\")\n",
    "    filteredCorrDf = corrDf.filter(col(\"feature_strength\").isin(\"Strong\", \"Moderate\"))\n",
    "    filteredCorrDf.orderBy(abs(col(\"pearson_correlation\")).desc()).show(truncate=False)\n",
    "    strongCorrFeatures = filteredCorrDf.select(\"feature\").rdd.flatMap(lambda x: x).collect()\n",
    "    return strongCorrFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "40a457f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_assemble(churnDataframe, featureCols, \n",
    "                     trainSplit:float, testSplit:float) -> tuple[DataFrame, DataFrame, bool]:\n",
    "    \"\"\"\n",
    "    Combines features into vector format and splits data into training and test sets.\n",
    "    Creates feature vectors for XGBoost and performs train-test split as specified.\n",
    "    Args:\n",
    "        churnDataframe: Input DataFrame containing features\n",
    "        featureCols: List of feature column names to assemble\n",
    "        trainSplit: Proportion of data for training (%)\n",
    "        testSplit: Proportion of data for testing (%)\n",
    "    Returns:\n",
    "        trainDf: Training dataset with feature vectors\n",
    "        testDf: Test dataset with feature vectors\n",
    "    \"\"\"\n",
    "    print(\"Assembling features...\")\n",
    "    print(\"Combining all features into single vector...\")\n",
    "    assembler_all = VectorAssembler(inputCols = featureCols, outputCol=\"features\") \n",
    "    pipeline = Pipeline(stages=[assembler_all])\n",
    "    vectorDf = pipeline.fit(churnDataframe).transform(churnDataframe)\n",
    "    print(\"Splitting data...\")\n",
    "    print(f\"{trainSplit*100}/{testSplit*100} Test Train Split...\")\n",
    "    trainDf, testDf = vectorDf.randomSplit([trainSplit, testSplit], seed=42)\n",
    "    return trainDf, testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "2fe766d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(trainDf, testDf, targetCol) -> tuple[DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Trains and evaluates a baseline XGBoost model using all available features.\n",
    "    Returns the trained model and its F1 score on the test set.\n",
    "    Args:\n",
    "        trainDf: Training DataFrame containing feature vectors\n",
    "        testDf: Test DataFrame for model evaluation\n",
    "        targetCol: Name of the target/label column\n",
    "    Returns:\n",
    "        xgbModel: Trained XGBoost model\n",
    "        baselineF1Score: F1 score achieved on test data\n",
    "    \"\"\"\n",
    "    xgbClassifier = SparkXGBClassifier(features_col=\"features\", label_col=targetCol, \n",
    "                                       prediction_col=\"prediction\", num_workers=cores)\n",
    "    xgbModel = xgbClassifier.fit(trainDf)\n",
    "    predictions = xgbModel.transform(testDf)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=targetCol, \n",
    "                                                  predictionCol=\"prediction\",metricName=\"f1\")\n",
    "    baseF1Score = round(evaluator.evaluate(predictions),5)\n",
    "    precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
    "    recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
    "    \n",
    "    print(f\"Base Model F1 Score: {round(baseF1Score, 5)}\")\n",
    "    print(f\"Base Model Precision: {round(precision, 5)}\")\n",
    "    print(f\"Base Model Recall: {round(recall, 5)}\\n\")\n",
    "    return xgbModel, baseF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "6f22c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_information_gain(model, featureCols) -> list:\n",
    "    importances = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    print(\"All Feature Importances by Information Gain:\")\n",
    "\n",
    "    featureMap = {f\"f{i}\": name for i, name in enumerate(featureCols)}\n",
    "    sortedImportances = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    featureDict = {}\n",
    "    for feature, score in sortedImportances:\n",
    "        feature = featureMap.get(feature, feature)\n",
    "        featureDict[feature] = score\n",
    "    scoreList = [[featureMap.get(feat, feat), score] for feat, score in sortedImportances]\n",
    "    allFeatureGainDf = spark.createDataFrame(scoreList, [\"feature\", \"gain\"])\n",
    "    allFeatureGainDf = allFeatureGainDf.withColumn(\"gain_strength\",\n",
    "         when(abs(col(\"gain\")) > 30, \"Strong\")\n",
    "        .when(abs(col(\"gain\")) > 10, \"Moderate\")\n",
    "        .otherwise(\"Weak\"))\n",
    "    allFeatureGainDf.show(n=allFeatureGainDf.count(), truncate=False)\n",
    "    print(\"Features with stronger information gain scores...\")\n",
    "    strongGainDf = allFeatureGainDf.filter(col(\"gain_strength\").isin(\"Strong\", \"Moderate\"))\n",
    "    strongGainDf.orderBy(abs(col(\"gain_strength\")).desc()).show(truncate=False)\n",
    "    strongGainFeatures = strongGainDf.select(\"feature\").rdd.flatMap(lambda x: x).collect()\n",
    "    return strongGainFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "82037007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strong_features(corrFeatures: list, gainFeatures: list) -> list:\n",
    "        strongFeatures = list(set(corrFeatures + gainFeatures))\n",
    "        print(f\"Strong feature list: {strongFeatures}\")\n",
    "        return strongFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "e9cf5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cross_validation(trainDf, testDf, featureCols, \n",
    "                           TargetCol, modelType = \"GBT\", k=5)-> tuple[GBTClassificationModel, float]:\n",
    "    print(f\"Running {k}-fold CV on training data using model: {modelType}\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols= featureCols, outputCol=\"strongFeatures\")\n",
    "    train = assembler.transform(trainDf).select(\"strongFeatures\", TargetCol)\n",
    "    test = assembler.transform(testDf).select(\"strongFeatures\", TargetCol)\n",
    "\n",
    "    model = GBTClassifier(labelCol=TargetCol, featuresCol=\"strongFeatures\")\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(model.maxDepth, [3, 5]) \\\n",
    "        .addGrid(model.maxIter, [10, 20]) \\\n",
    "        .addGrid(model.stepSize, [0.05, 0.1]) \\\n",
    "        .addGrid(model.subsamplingRate, [0.8, 1.0]) \\\n",
    "        .build()\n",
    "\n",
    "    # Evaluator\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=TargetCol,\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"f1\")\n",
    "\n",
    "    # CrossValidator\n",
    "    cv = CrossValidator(\n",
    "        estimator=model,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=k,\n",
    "        parallelism=cores)\n",
    "\n",
    "    cvModel = cv.fit(train)\n",
    "    predictions = cvModel.transform(test)\n",
    "    print(predictions)\n",
    "    optimisedF1Score = round(evaluator.evaluate(predictions),5)\n",
    "    precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
    "    recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
    "\n",
    "    print(f\"Optimised Model F1 Score: {round(optimisedF1Score, 5)}\")\n",
    "    print(f\"Optimised Precision: {round(precision, 5)}\")\n",
    "    print(f\"Optimised Recall: {round(recall, 5)}\\n\")\n",
    "    \n",
    "    return cvModel.bestModel, optimisedF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "fa7f4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(baselineF1, optimisedF1, baseModel, \n",
    "                      optimisedModel, strongFeatures, allFeatures) -> tuple[ClassificationModel, str, list]:\n",
    "    print(f\"Base model F1 Score: {baselineF1}\")\n",
    "    print(f\"Optimised model F1 Score: {optimisedF1}\")\n",
    "    if optimisedF1 > baselineF1:\n",
    "        model = optimisedModel\n",
    "        modelType = \"optimised\"\n",
    "        features = strongFeatures\n",
    "        print(f\"Optimised model selected...\")\n",
    "    else:\n",
    "        model = baseModel\n",
    "        modelType = \"base\"\n",
    "        features = allFeatures\n",
    "        print(f\"Base model selected...\")\n",
    "    return model, modelType, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "40d0abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_churn_score(modelType, model, \n",
    "                        churnDataUnlabelled: DataFrame, features, reservedFlag) -> None:\n",
    "\n",
    "    if modelType == \"optimised\":\n",
    "        feature_vector_col = \"strongFeatures\"\n",
    "        selectedFeatures = ['actual_churn_risk_score'] + features\n",
    "    else:\n",
    "        feature_vector_col = \"features\"\n",
    "        selectedFeatures = ['actual_churn_risk_score'] + features\n",
    "\n",
    "    churnDataUnlabelled = churnDataUnlabelled.select(*[feature for feature in selectedFeatures if feature in churnDataUnlabelled.columns])\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=feature_vector_col)\n",
    "    churnunlabeledPredict = assembler.transform(churnDataUnlabelled)\n",
    "    predictions = model.transform(churnunlabeledPredict)\n",
    "\n",
    "    predictions = predictions.withColumnRenamed(\"prediction\", \"predicted_churn_risk_score\")\n",
    "    \n",
    "    # Determine which label column to use\n",
    "    if \"actual_churn_risk_score\" in predictions.columns:\n",
    "        label_col = \"actual_churn_risk_score\"\n",
    "    elif \"churn_risk_score\" in predictions.columns:\n",
    "        label_col = \"churn_risk_score\"\n",
    "\n",
    "    if label_col:\n",
    "        selected_df = predictions.select(label_col, \"predicted_churn_risk_score\")\n",
    "        print(f\"\\n----- Prediction Results ({modelType} model using '{label_col}') -----\")\n",
    "        selected_df = selected_df.withColumn(\"predicted_churn_risk_score\", selected_df[\"predicted_churn_risk_score\"].cast(IntegerType()))\n",
    "        selected_df.show(truncate=False)\n",
    "\n",
    "        correct = predictions.filter(col(label_col) == col(\"predicted_churn_risk_score\")).count()\n",
    "        total = predictions.count()\n",
    "        accuracy = correct / total if total else 0\n",
    "        print(f\"Prediction Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    else:\n",
    "        print(\"\\nNo label column found ('actual_churn_risk_score' or 'churn_risk_score') in data.\")\n",
    "        predictions.select(\"predicted_churn_risk_score\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "f6d4bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_ml_pipeline() -> None:\n",
    "    \n",
    "    startTime = time.time()\n",
    "    \n",
    "    print(\"Spark session created: ChurnXGBoost \")\n",
    "    print()\n",
    "\n",
    "    randomId = uuid.uuid4()\n",
    "    stringID = str(randomId)\n",
    "    print(f\"Executing Machine Learning Pipeline...\")\n",
    "    print() # Creates a gap for for clean logging output\n",
    "    print(f\"Run ID: {stringID}\")\n",
    "    print()\n",
    "\n",
    "    print(\"***** DATA PREPARATION *****\", end=\"\\n\")\n",
    "    print()\n",
    "    \n",
    "    print(\"===== Combine Cleaned Files and Load Data =====\", end=\"\\n\")\n",
    "    churnData = concatenate_clean_churn_files()\n",
    "\n",
    "    print(\"===== Cleaned Churn Metadata =====\", end=\"\\n\")\n",
    "    get_churn_metadata(churnData)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Split Dataframe by labels =====\", end=\"\\n\")\n",
    "    churnDataLabelled, churnDataUnlabelled, reservedFlag = split_dataframe(churnData)\n",
    "    print()\n",
    "\n",
    "    print(\"***** TRAIN AND SCORE BASELINE MODEL *****\", end=\"\\n\")\n",
    "    print()\n",
    "\n",
    "    print(\"===== Baseline Model: Separate Feature and Target Column/s =====\", end=\"\\n\")\n",
    "    allFeatures, targetCol = separate_feature_target_cols(churnDataLabelled)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Baseline Model: Correlation Analysis =====\", end=\"\\n\")\n",
    "    strongCorrFeatures = correlation_analysis(churnDataLabelled, allFeatures, targetCol, 0.2, 0.4)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Baseline Model: Assemble Features =====\", end=\"\\n\")\n",
    "    trainSetDf, testSetDf = feature_assemble(churnDataLabelled, allFeatures, trainSplit=0.8, testSplit=0.2)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Baseline Model: Create and Score =====\",end=\"\\n\")\n",
    "    baseModel, baselineF1Score = baseline_model(trainSetDf, testSetDf, targetCol)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Baseline Model: Feature Information Gain =====\",end=\"\\n\")\n",
    "    strongGainFeatures = feature_information_gain(baseModel, allFeatures)\n",
    "    print()\n",
    "\n",
    "    print(\"***** TRAIN AND SCORE OPTIMISED MODEL *****\",end=\"\\n\")\n",
    "    print()\n",
    "\n",
    "    print(\"===== Optimised Model: Strongest Features =====\",end=\"\\n\")\n",
    "    strongFeatures = get_strong_features(strongCorrFeatures, strongGainFeatures)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Optimised Model: K-Fold Cross Validation =====\",end=\"\\n\")\n",
    "    optimisedModel, optimisedF1Score = kfold_cross_validation(trainSetDf, testSetDf, strongFeatures, targetCol)\n",
    "    print()\n",
    "\n",
    "    print(\"***** SELECT BEST MODEL AND PREDICT UNSEEN DATA *****\",end=\"\\n\")\n",
    "    print()\n",
    "\n",
    "    print(\"===== Select Best Model =====\",end=\"\\n\")\n",
    "    bestModel, modelType, features = select_best_model(baselineF1Score, optimisedF1Score,\n",
    "                                             baseModel, optimisedModel, strongFeatures, allFeatures)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Apply Model to Unseen Data =====\",end=\"\\n\")\n",
    "    predict_churn_score(modelType, bestModel, churnDataUnlabelled, features, reservedFlag)\n",
    "    \n",
    "    endTime = time.time()\n",
    "    pipelineRunTime = endTime - startTime\n",
    "    print(f\"Machine learning pipeline complete. Run time: {timedelta(seconds=int(pipelineRunTime))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "085b9b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created: ChurnXGBoost \n",
      "\n",
      "Executing Machine Learning Pipeline...\n",
      "\n",
      "Run ID: 5129a1b6-6dc3-4b16-89ac-e5491997482c\n",
      "\n",
      "***** DATA PREPARATION *****\n",
      "\n",
      "===== Combine Cleaned Files and Load Data =====\n",
      "All available cleaned churn csv files: ['churn_clean.csv']\n",
      "===== Cleaned Churn Metadata =====\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 25 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   age                           20000 non-null  int32  \n",
      " 1   gender                        20000 non-null  int32  \n",
      " 2   region_category               20000 non-null  int32  \n",
      " 3   membership_category           20000 non-null  int32  \n",
      " 4   joined_through_referral       20000 non-null  int32  \n",
      " 5   preferred_offer_types         20000 non-null  int32  \n",
      " 6   medium_of_operation           20000 non-null  int32  \n",
      " 7   internet_option               20000 non-null  int32  \n",
      " 8   days_since_last_login         20000 non-null  int32  \n",
      " 9   avg_time_spent                20000 non-null  float64\n",
      " 10  avg_transaction_value         20000 non-null  float64\n",
      " 11  avg_frequency_login_days      20000 non-null  int32  \n",
      " 12  points_in_wallet              20000 non-null  float64\n",
      " 13  used_special_discount         20000 non-null  int32  \n",
      " 14  offer_application_preference  20000 non-null  int32  \n",
      " 15  past_complaint                20000 non-null  int32  \n",
      " 16  complaint_status              20000 non-null  int32  \n",
      " 17  feedback                      20000 non-null  int32  \n",
      " 18  join_year                     20000 non-null  int32  \n",
      " 19  join_month                    20000 non-null  int32  \n",
      " 20  join_day                      20000 non-null  int32  \n",
      " 21  last_visit_hour               20000 non-null  int32  \n",
      " 22  last_visit_min                20000 non-null  int32  \n",
      " 23  last_visit_sec                20000 non-null  int32  \n",
      " 24  churn_risk_score              20000 non-null  int32  \n",
      "dtypes: float64(3), int32(22)\n",
      "memory usage: 2.1 MB\n",
      "\n",
      "===== Split Dataframe by labels =====\n",
      "no empty churn_risk_score rows...\n",
      "Reserving 1% of labeled data for demo predictions...\n",
      "Reserve dataframe with 208 rows\n",
      "\n",
      "***** TRAIN AND SCORE BASELINE MODEL *****\n",
      "\n",
      "===== Baseline Model: Separate Feature and Target Column/s =====\n",
      "All Feature Columns: ['age', 'gender', 'region_category', 'membership_category', 'joined_through_referral', 'preferred_offer_types', 'medium_of_operation', 'internet_option', 'days_since_last_login', 'avg_time_spent', 'avg_transaction_value', 'avg_frequency_login_days', 'points_in_wallet', 'used_special_discount', 'offer_application_preference', 'past_complaint', 'complaint_status', 'feedback', 'join_year', 'join_month', 'join_day', 'last_visit_hour', 'last_visit_min', 'last_visit_sec']\n",
      "Target Column: churn_risk_score\n",
      "\n",
      "===== Baseline Model: Correlation Analysis =====\n",
      "Begin correlation analysis of all features...\n",
      "\n",
      "All Correlation Scores:\n",
      "+----------------------------+---------------------+----------------+\n",
      "|feature                     |pearson_correlation  |feature_strength|\n",
      "+----------------------------+---------------------+----------------+\n",
      "|membership_category         |-0.4644145540752387  |Strong          |\n",
      "|avg_transaction_value       |-0.21670086929826687 |Moderate        |\n",
      "|points_in_wallet            |-0.2077226314261469  |Moderate        |\n",
      "|avg_frequency_login_days    |0.14246476558304483  |Weak            |\n",
      "|feedback                    |-0.12729664500324794 |Weak            |\n",
      "|preferred_offer_types       |0.04277998339325452  |Weak            |\n",
      "|joined_through_referral     |0.030535720934640978 |Weak            |\n",
      "|days_since_last_login       |0.023087402336053473 |Weak            |\n",
      "|past_complaint              |0.019461328637430487 |Weak            |\n",
      "|used_special_discount       |-0.01744458886984828 |Weak            |\n",
      "|offer_application_preference|-0.014619386210010634|Weak            |\n",
      "|avg_time_spent              |-0.01029762218541746 |Weak            |\n",
      "|join_day                    |0.009988916581946618 |Weak            |\n",
      "|medium_of_operation         |0.008906670793867269 |Weak            |\n",
      "|age                         |0.00868741703381818  |Weak            |\n",
      "|join_month                  |0.00855686069677683  |Weak            |\n",
      "|complaint_status            |0.008112710188245081 |Weak            |\n",
      "|region_category             |-0.007180234415781032|Weak            |\n",
      "|internet_option             |-0.005807078282886454|Weak            |\n",
      "|last_visit_sec              |0.005800782990621628 |Weak            |\n",
      "|gender                      |-0.005476132649570174|Weak            |\n",
      "|last_visit_hour             |-0.005194617790894275|Weak            |\n",
      "|last_visit_min              |0.003286625495654285 |Weak            |\n",
      "|join_year                   |0.003178843031535454 |Weak            |\n",
      "+----------------------------+---------------------+----------------+\n",
      "\n",
      "\n",
      "Features displaying the strongest predictive signal:\n",
      "+---------------------+--------------------+----------------+\n",
      "|feature              |pearson_correlation |feature_strength|\n",
      "+---------------------+--------------------+----------------+\n",
      "|membership_category  |-0.4644145540752387 |Strong          |\n",
      "|avg_transaction_value|-0.21670086929826687|Moderate        |\n",
      "|points_in_wallet     |-0.2077226314261469 |Moderate        |\n",
      "+---------------------+--------------------+----------------+\n",
      "\n",
      "\n",
      "===== Baseline Model: Assemble Features =====\n",
      "Assembling features...\n",
      "Combining all features into single vector...\n",
      "Splitting data...\n",
      "80.0/20.0 Test Train Split...\n",
      "\n",
      "===== Baseline Model: Create and Score =====\n",
      "Base Model F1 Score: 0.93497\n",
      "Base Model Precision: 0.93635\n",
      "Base Model Recall: 0.93513\n",
      "\n",
      "\n",
      "===== Baseline Model: Feature Information Gain =====\n",
      "All Feature Importances by Information Gain:\n",
      "+----------------------------+------------------+-------------+\n",
      "|feature                     |gain              |gain_strength|\n",
      "+----------------------------+------------------+-------------+\n",
      "|points_in_wallet            |47.046443939208984|Strong       |\n",
      "|membership_category         |46.6749382019043  |Strong       |\n",
      "|feedback                    |10.035938262939453|Moderate     |\n",
      "|avg_transaction_value       |3.964210271835327 |Weak         |\n",
      "|offer_application_preference|2.4240946769714355|Weak         |\n",
      "|complaint_status            |2.29445743560791  |Weak         |\n",
      "|medium_of_operation         |2.216881275177002 |Weak         |\n",
      "|last_visit_hour             |2.1581339836120605|Weak         |\n",
      "|avg_time_spent              |2.0533933639526367|Weak         |\n",
      "|age                         |2.0245985984802246|Weak         |\n",
      "|avg_frequency_login_days    |2.0087668895721436|Weak         |\n",
      "|internet_option             |1.9936082363128662|Weak         |\n",
      "|gender                      |1.993508219718933 |Weak         |\n",
      "|join_month                  |1.9845401048660278|Weak         |\n",
      "|days_since_last_login       |1.9586044549942017|Weak         |\n",
      "|last_visit_min              |1.9578497409820557|Weak         |\n",
      "|join_day                    |1.9422471523284912|Weak         |\n",
      "|join_year                   |1.9413337707519531|Weak         |\n",
      "|last_visit_sec              |1.9383958578109741|Weak         |\n",
      "|preferred_offer_types       |1.8952021598815918|Weak         |\n",
      "|region_category             |1.8513678312301636|Weak         |\n",
      "|joined_through_referral     |1.7452489137649536|Weak         |\n",
      "|past_complaint              |1.723747730255127 |Weak         |\n",
      "|used_special_discount       |1.506328821182251 |Weak         |\n",
      "+----------------------------+------------------+-------------+\n",
      "\n",
      "Features with stronger information gain scores...\n",
      "+-------------------+------------------+-------------+\n",
      "|feature            |gain              |gain_strength|\n",
      "+-------------------+------------------+-------------+\n",
      "|feedback           |10.035938262939453|Moderate     |\n",
      "|points_in_wallet   |47.046443939208984|Strong       |\n",
      "|membership_category|46.6749382019043  |Strong       |\n",
      "+-------------------+------------------+-------------+\n",
      "\n",
      "\n",
      "***** TRAIN AND SCORE OPTIMISED MODEL *****\n",
      "\n",
      "===== Optimised Model: Strongest Features =====\n",
      "Strong feature list: ['points_in_wallet', 'membership_category', 'feedback', 'avg_transaction_value']\n",
      "\n",
      "===== Optimised Model: K-Fold Cross Validation =====\n",
      "Running 5-fold CV on training data using model: GBT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[strongFeatures: vector, churn_risk_score: int, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Optimised Model F1 Score: 0.93373\n",
      "Optimised Precision: 0.93461\n",
      "Optimised Recall: 0.93385\n",
      "\n",
      "\n",
      "***** SELECT BEST MODEL AND PREDICT UNSEEN DATA *****\n",
      "\n",
      "===== Select Best Model =====\n",
      "Base model F1 Score: 0.93497\n",
      "Optimised model F1 Score: 0.93373\n",
      "Base model selected...\n",
      "\n",
      "===== Apply Model to Unseen Data =====\n",
      "\n",
      "----- Prediction Results (base model using 'actual_churn_risk_score') -----\n",
      "+-----------------------+--------------------------+\n",
      "|actual_churn_risk_score|predicted_churn_risk_score|\n",
      "+-----------------------+--------------------------+\n",
      "|1                      |1                         |\n",
      "|0                      |0                         |\n",
      "|0                      |0                         |\n",
      "|1                      |1                         |\n",
      "|0                      |0                         |\n",
      "|1                      |1                         |\n",
      "|1                      |1                         |\n",
      "|1                      |1                         |\n",
      "|0                      |0                         |\n",
      "|1                      |1                         |\n",
      "|1                      |1                         |\n",
      "|0                      |0                         |\n",
      "|1                      |0                         |\n",
      "|1                      |1                         |\n",
      "|0                      |0                         |\n",
      "|0                      |0                         |\n",
      "|1                      |1                         |\n",
      "|1                      |1                         |\n",
      "|1                      |1                         |\n",
      "|1                      |1                         |\n",
      "+-----------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Prediction Accuracy: 93.27% (194/208)\n",
      "Machine learning pipeline complete. Run time: 0:00:47\n"
     ]
    }
   ],
   "source": [
    "execute_ml_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
