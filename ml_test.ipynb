{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "4bf9fdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.7.6 in /opt/conda/lib/python3.10/site-packages (1.7.6)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xgboost==1.7.6) (1.26.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost==1.7.6) (1.11.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost==1.7.6\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.functions import col, abs\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit\n",
    "import uuid\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "0ab2b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ChurnXGBoost\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "4de1d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_churn_data(fileName: str) -> DataFrame:\n",
    "    try:\n",
    "        churnDataframe = spark.read.csv(fileName, header=True, inferSchema=True)\n",
    "        print(\"Churn data loaded...\")\n",
    "        return churnDataframe\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "254013f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_churn_metadata(churnDataFrame) -> None:\n",
    "    tempPandasDf = churnDataFrame.toPandas() # Localised spark df conversion to pandas\n",
    "    tempPandasDf.info() # Display metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "319b2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(churnDataFrame):\n",
    "    churnlabeled = churnDataFrame.filter(col('churn_risk_score').isNotNull())\n",
    "    churnunlabeled = churnDataFrame.filter(col('churn_risk_score').isNull())\n",
    "    reserve = 0.01\n",
    "    reservedFlag = False\n",
    "    if churnunlabeled.count() == 0:\n",
    "        reservedFlag = True\n",
    "        print('no empty churn_risk_score rows...')\n",
    "        print(f\"Reserving {int(1)}% of labeled data for demo predictions...\")\n",
    "        trainDf, reservedDf = churnlabeled.randomSplit([1 - reserve, reserve], seed=42)\n",
    "        churnlabeled = trainDf\n",
    "        churnunlabeled = reservedDf\n",
    "        print(f\"Reserve dataframe with {churnunlabeled.count()} rows\")\n",
    "        churnunlabeled = churnunlabeled.withColumnRenamed(\"churn_risk_score\", \"actual_churn_risk_score\")\n",
    "        churnunlabeled = churnunlabeled.withColumn(\"predicted_churn_risk_score\", lit(None))\n",
    "    return churnlabeled, churnunlabeled, reservedFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "f0a958c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_feature_target_cols(churnDataFrame) -> [list,str]:\n",
    "    featureCols = churnDataFrame.columns[:-1]\n",
    "    targetCol = churnDataFrame.columns[-1]\n",
    "    print(f\"All Feature Columns: {featureCols}\")\n",
    "    print()\n",
    "    print(f\"Target Column: {targetCol}\")\n",
    "    return featureCols, targetCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "e63d0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(churnDataFrame: DataFrame, featureCols: list,\n",
    "                         targetCol: str, moderateVal: float, strongVal: float) -> None:\n",
    "    print(\"Begin correlation analysis of all features...\\n\")\n",
    "\n",
    "    corrVals = []\n",
    "    for feature in featureCols:\n",
    "        corr = churnDataFrame.stat.corr(feature, targetCol)\n",
    "        corrVals.append((feature, corr))\n",
    "\n",
    "    corrDf = spark.createDataFrame(corrVals, [\"feature\", \"pearson_correlation\"]) \\\n",
    "                  .orderBy(abs(col(\"pearson_correlation\")).desc())\n",
    "\n",
    "    corrDf = corrDf.withColumn(\"feature_strength\",\n",
    "         when(abs(col(\"pearson_correlation\")) > strongVal, \"Strong\")\n",
    "        .when(abs(col(\"pearson_correlation\")) > moderateVal, \"Moderate\")\n",
    "        .otherwise(\"Weak\"))\n",
    "    print(\"All Correlation Scores:\")\n",
    "    corrDf.show(n=corrDf.count(), truncate=False)\n",
    "    print()\n",
    "    print(\"Features displaying the strongest predictive signal:\")\n",
    "    filteredCorrDf = corrDf.filter(col(\"feature_strength\").isin(\"Strong\", \"Moderate\"))\n",
    "    filteredCorrDf.orderBy(abs(col(\"pearson_correlation\")).desc()).show(truncate=False)\n",
    "    strongCorrFeatures = filteredCorrDf.select(\"feature\").rdd.flatMap(lambda x: x).collect()\n",
    "    return strongCorrFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "40a457f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_assemble(churnDataframe, featureCols, trainSplit:float, testSplit:float) -> [DataFrame, DataFrame]:\n",
    "    print(\"Assembling features...\")\n",
    "    print(\"Combining all features into single vector...\")\n",
    "    assembler_all = VectorAssembler(inputCols = featureCols,\n",
    "                                    outputCol=\"features\") # Combine all features into a vector\n",
    "    pipeline = Pipeline(stages=[assembler_all])\n",
    "    vectorDf = pipeline.fit(churnDataframe).transform(churnDataframe)\n",
    "    print(\"Splitting data...\")\n",
    "    print(f\"{trainSplit*100}/{testSplit*100} Test Train Split...\")\n",
    "    trainDf, testDf = vectorDf.randomSplit([trainSplit, testSplit], seed=42)\n",
    "    return trainDf, testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "2fe766d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(trainDf, testDf, targetCol) -> [DataFrame, str]:\n",
    "    xgbClassifier = SparkXGBClassifier(\n",
    "            features_col=\"features\",\n",
    "            label_col=targetCol,\n",
    "            prediction_col=\"prediction\",\n",
    "            num_workers=spark.sparkContext.defaultParallelism )\n",
    "    xgbModel = xgbClassifier.fit(trainDf)\n",
    "    predictions = xgbModel.transform(testDf)\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=targetCol,\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"f1\")\n",
    "    baselineF1Score = round(evaluator.evaluate(predictions),5)\n",
    "    print(f\"Baseline model F1 Score - All Features: {baselineF1Score}\")\n",
    "    return xgbModel, baselineF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "6f22c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_information_gain(model, featureCols) -> [str]:\n",
    "    importances = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    print(\"All Feature Importances by Information Gain:\")\n",
    "\n",
    "    featureMap = {f\"f{i}\": name for i, name in enumerate(featureCols)}\n",
    "    sortedImportances = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    featureDict = {}\n",
    "    for feature, score in sortedImportances:\n",
    "        feature = featureMap.get(feature, feature)\n",
    "        featureDict[feature] = score\n",
    "    scoreList = [[featureMap.get(feat, feat), score] for feat, score in sortedImportances]\n",
    "    allFeatureGainDf = spark.createDataFrame(scoreList, [\"feature\", \"gain\"])\n",
    "    allFeatureGainDf = allFeatureGainDf.withColumn(\"gain_strength\",\n",
    "         when(abs(col(\"gain\")) > 30, \"Strong\")\n",
    "        .when(abs(col(\"gain\")) > 10, \"Moderate\")\n",
    "        .otherwise(\"Weak\"))\n",
    "    allFeatureGainDf.show(n=allFeatureGainDf.count(), truncate=False)\n",
    "    print(\"Features with stronger information gain scores...\")\n",
    "    strongGainDf = allFeatureGainDf.filter(col(\"gain_strength\").isin(\"Strong\", \"Moderate\"))\n",
    "    strongGainDf.orderBy(abs(col(\"gain_strength\")).desc()).show(truncate=False)\n",
    "    strongGainFeatures = strongGainDf.select(\"feature\").rdd.flatMap(lambda x: x).collect()\n",
    "    return strongGainFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "82037007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strong_features(corrFeatures: list, gainFeatures: list) -> [str]:\n",
    "        strongFeatures = list(set(corrFeatures + gainFeatures))\n",
    "        print(f\"Strong feature list: {strongFeatures}\")\n",
    "        return strongFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cross_validation(trainDf, testDf, featureCols, TargetCol, modelType = \"GBT\", k=3):\n",
    "    print(f\"Running {k}-fold CV on training data using model: {modelType}\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols= featureCols, outputCol=\"strongFeatures\")\n",
    "    train = assembler.transform(trainDf).select(\"strongFeatures\", TargetCol)\n",
    "    test = assembler.transform(testDf).select(\"strongFeatures\", TargetCol)\n",
    "\n",
    "    model = GBTClassifier(labelCol=TargetCol, featuresCol=\"strongFeatures\")\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(model.maxDepth, [3, 5]) \\\n",
    "        .addGrid(model.maxIter, [10, 20]) \\\n",
    "        .addGrid(model.stepSize, [0.05, 0.1]) \\\n",
    "        .addGrid(model.subsamplingRate, [0.8, 1.0]) \\\n",
    "        .build()\n",
    "\n",
    "    # Evaluator\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=TargetCol,\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"f1\")\n",
    "\n",
    "    # CrossValidator\n",
    "    cv = CrossValidator(\n",
    "        estimator=model,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=k,\n",
    "        parallelism=spark.sparkContext.defaultParallelism)\n",
    "\n",
    "    cvModel = cv.fit(train)\n",
    "    predictions = cvModel.transform(test)\n",
    "    print(predictions)\n",
    "    optimisedF1Score = round(evaluator.evaluate(predictions),5)\n",
    "    print(f\"F1 score on test data: {optimisedF1Score}\")\n",
    "    return cvModel.bestModel, optimisedF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "fa7f4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(baselineF1, optimisedF1, baseModel, optimisedModel, strongFeatures, allFeatures):\n",
    "    print(f\"Base model F1 Score: {baselineF1}\")\n",
    "    print(f\"Optimised model F1 Score: {optimisedF1}\")\n",
    "    if optimisedF1 > baselineF1:\n",
    "        model = optimisedModel\n",
    "        modelType = \"optimised\"\n",
    "        features = strongFeatures\n",
    "        print(f\"Optimised model selected...\")\n",
    "    else:\n",
    "        model = baseModel\n",
    "        modelType = \"base\"\n",
    "        features = allFeatures\n",
    "        print(f\"Base model selected...\")\n",
    "    return model, modelType, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_churn_score(modelType, model, churnDataUnlabelled: DataFrame, features, reservedFlag):\n",
    "\n",
    "    if modelType == \"optimised\":\n",
    "        feature_vector_col = \"strongFeatures\"\n",
    "        cols_to_select = ['actual_churn_risk_score'] + features\n",
    "    else:\n",
    "        feature_vector_col = \"features\"\n",
    "        cols_to_select = ['actual_churn_risk_score'] + features\n",
    "\n",
    "    churnDataUnlabelled = churnDataUnlabelled.select(*[c for c in cols_to_select if c in churnDataUnlabelled.columns])\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=feature_vector_col)\n",
    "    churnunlabeledPredict = assembler.transform(churnDataUnlabelled)\n",
    "    predictions = model.transform(churnunlabeledPredict)\n",
    "\n",
    "    predictions = predictions.withColumnRenamed(\"prediction\", \"predicted_churn_risk_score\")\n",
    "\n",
    "    selected_df = predictions.select(\"actual_churn_risk_score\", \"predicted_churn_risk_score\")\n",
    "    print(f\"\\n----- Prediction Results ({modelType} model) -----\")\n",
    "    selected_df.show(truncate=False)\n",
    "\n",
    "    if \"actual_churn_risk_score\" in predictions.columns:\n",
    "        correct = predictions.filter(col(\"actual_churn_risk_score\") == col(\"predicted_churn_risk_score\")).count()\n",
    "        total = predictions.count()\n",
    "        accuracy = correct / total if total else 0\n",
    "        print(f\"Prediction Accuracy: {accuracy:.2%} ({correct}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_ml_pipeline():\n",
    "\n",
    "    print(\"Spark session created: ChurnXGBoost \")\n",
    "    print()\n",
    "\n",
    "    randomId = uuid.uuid4()\n",
    "    stringID = str(randomId)\n",
    "    print(f\"Executing Machine Learning Pipeline...\")\n",
    "    print() # Creates a gap for for clean logging output\n",
    "    print(f\"Run ID: {stringID}\")\n",
    "    print()\n",
    "\n",
    "    print(\"***** DATA PREPARATION *****\", end=\"\\n\")\n",
    "    print()\n",
    "\n",
    "    print(\"===== Load Cleaned Data =====\", end=\"\\n\")\n",
    "    churnData = get_churn_data(\"churn_clean.csv\")\n",
    "    print()\n",
    "\n",
    "    print(\"===== Cleaned Churn Metadata =====\", end=\"\\n\")\n",
    "    get_churn_metadata(churnData)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Split Dataframe by labels =====\", end=\"\\n\")\n",
    "    churnDataLabelled, churnDataUnlabelled, reservedFlag = split_dataframe(churnData)\n",
    "    print()\n",
    "\n",
    "    print(\"***** TRAIN AND SCORE BASELINE MODEL *****\", end=\"\\n\")\n",
    "    print()\n",
    "\n",
    "    print(\"===== Baseline Model: Separate Feature and Target Column/s =====\", end=\"\\n\")\n",
    "    allFeatures, targetCol = separate_feature_target_cols(churnDataLabelled)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Baseline Model: Correlation Analysis =====\", end=\"\\n\")\n",
    "    strongCorrFeatures = correlation_analysis(churnDataLabelled, allFeatures, targetCol, 0.2, 0.4)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Baseline Model: Assemble Features =====\", end=\"\\n\")\n",
    "    trainSetDf, testSetDf = feature_assemble(churnDataLabelled, allFeatures, trainSplit=0.8, testSplit=0.2)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Baseline Model: Create and Score =====\",end=\"\\n\")\n",
    "    baseModel, baselineF1Score = baseline_model(trainSetDf, testSetDf, targetCol)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Baseline Model: Feature Information Gain =====\",end=\"\\n\")\n",
    "    strongGainFeatures = feature_information_gain(baseModel, allFeatures)\n",
    "    print()\n",
    "\n",
    "    print(\"***** TRAIN AND SCORE OPTIMISED MODEL *****\",end=\"\\n\")\n",
    "    print()\n",
    "\n",
    "    print(\"===== Optimised Model: Strongest Features =====\",end=\"\\n\")\n",
    "    strongFeatures = get_strong_features(strongCorrFeatures, strongGainFeatures)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Optimised Model: K-Fold Cross Validation =====\",end=\"\\n\")\n",
    "    optimisedModel, optimisedF1Score = kfold_cross_validation(trainSetDf, testSetDf, strongFeatures, targetCol)\n",
    "    print()\n",
    "\n",
    "    print(\"***** SELECT BEST MODEL AND PREDICT UNSEEN DATA *****\",end=\"\\n\")\n",
    "    print()\n",
    "\n",
    "    print(\"===== Select Best Model =====\",end=\"\\n\")\n",
    "    bestModel, modelType, features = select_best_model(baselineF1Score, optimisedF1Score,\n",
    "                                             baseModel, optimisedModel, strongFeatures, allFeatures)\n",
    "    print()\n",
    "\n",
    "    print(\"===== Apply Model to Unseen Data =====\",end=\"\\n\")\n",
    "    predict_churn_score(modelType, bestModel, churnDataUnlabelled, features, reservedFlag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "085b9b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created: ChurnXGBoost \n",
      "\n",
      "Executing Machine Learning Pipeline...\n",
      "\n",
      "Run ID: 9427b6f9-a144-4ee9-852f-19ccbdd3ee9b\n",
      "\n",
      "***** DATA PREPARATION *****\n",
      "\n",
      "===== Load Cleaned Data =====\n",
      "Churn data loaded...\n",
      "\n",
      "===== Cleaned Churn Metadata =====\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 25 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   age                           20000 non-null  int32  \n",
      " 1   gender                        20000 non-null  int32  \n",
      " 2   region_category               20000 non-null  int32  \n",
      " 3   membership_category           20000 non-null  int32  \n",
      " 4   joined_through_referral       20000 non-null  int32  \n",
      " 5   preferred_offer_types         20000 non-null  int32  \n",
      " 6   medium_of_operation           20000 non-null  int32  \n",
      " 7   internet_option               20000 non-null  int32  \n",
      " 8   days_since_last_login         20000 non-null  int32  \n",
      " 9   avg_time_spent                20000 non-null  float64\n",
      " 10  avg_transaction_value         20000 non-null  float64\n",
      " 11  avg_frequency_login_days      20000 non-null  int32  \n",
      " 12  points_in_wallet              20000 non-null  float64\n",
      " 13  used_special_discount         20000 non-null  int32  \n",
      " 14  offer_application_preference  20000 non-null  int32  \n",
      " 15  past_complaint                20000 non-null  int32  \n",
      " 16  complaint_status              20000 non-null  int32  \n",
      " 17  feedback                      20000 non-null  int32  \n",
      " 18  join_year                     20000 non-null  int32  \n",
      " 19  join_month                    20000 non-null  int32  \n",
      " 20  join_day                      20000 non-null  int32  \n",
      " 21  last_visit_hour               20000 non-null  int32  \n",
      " 22  last_visit_min                20000 non-null  int32  \n",
      " 23  last_visit_sec                20000 non-null  int32  \n",
      " 24  churn_risk_score              20000 non-null  int32  \n",
      "dtypes: float64(3), int32(22)\n",
      "memory usage: 2.1 MB\n",
      "\n",
      "===== Split Dataframe by labels =====\n",
      "no empty churn_risk_score rows...\n",
      "Reserving 1% of labeled data for demo predictions...\n",
      "Reserve dataframe with 208 rows\n",
      "\n",
      "***** TRAIN AND SCORE BASELINE MODEL *****\n",
      "\n",
      "===== Baseline Model: Separate Feature and Target Column/s =====\n",
      "All Feature Columns: ['age', 'gender', 'region_category', 'membership_category', 'joined_through_referral', 'preferred_offer_types', 'medium_of_operation', 'internet_option', 'days_since_last_login', 'avg_time_spent', 'avg_transaction_value', 'avg_frequency_login_days', 'points_in_wallet', 'used_special_discount', 'offer_application_preference', 'past_complaint', 'complaint_status', 'feedback', 'join_year', 'join_month', 'join_day', 'last_visit_hour', 'last_visit_min', 'last_visit_sec']\n",
      "\n",
      "Target Column: churn_risk_score\n",
      "\n",
      "===== Baseline Model: Correlation Analysis =====\n",
      "Begin correlation analysis of all features...\n",
      "\n",
      "All Correlation Scores:\n",
      "+----------------------------+---------------------+----------------+\n",
      "|feature                     |pearson_correlation  |feature_strength|\n",
      "+----------------------------+---------------------+----------------+\n",
      "|membership_category         |-0.4644145540752387  |Strong          |\n",
      "|avg_transaction_value       |-0.21670086929826687 |Moderate        |\n",
      "|points_in_wallet            |-0.2077226314261469  |Moderate        |\n",
      "|avg_frequency_login_days    |0.14246476558304483  |Weak            |\n",
      "|feedback                    |-0.12729664500324794 |Weak            |\n",
      "|preferred_offer_types       |0.04277998339325452  |Weak            |\n",
      "|joined_through_referral     |0.030535720934640978 |Weak            |\n",
      "|days_since_last_login       |0.023087402336053473 |Weak            |\n",
      "|past_complaint              |0.019461328637430487 |Weak            |\n",
      "|used_special_discount       |-0.01744458886984828 |Weak            |\n",
      "|offer_application_preference|-0.014619386210010634|Weak            |\n",
      "|avg_time_spent              |-0.01029762218541746 |Weak            |\n",
      "|join_day                    |0.009988916581946618 |Weak            |\n",
      "|medium_of_operation         |0.008906670793867269 |Weak            |\n",
      "|age                         |0.00868741703381818  |Weak            |\n",
      "|join_month                  |0.00855686069677683  |Weak            |\n",
      "|complaint_status            |0.008112710188245081 |Weak            |\n",
      "|region_category             |-0.007180234415781032|Weak            |\n",
      "|internet_option             |-0.005807078282886454|Weak            |\n",
      "|last_visit_sec              |0.005800782990621628 |Weak            |\n",
      "|gender                      |-0.005476132649570174|Weak            |\n",
      "|last_visit_hour             |-0.005194617790894275|Weak            |\n",
      "|last_visit_min              |0.003286625495654285 |Weak            |\n",
      "|join_year                   |0.003178843031535454 |Weak            |\n",
      "+----------------------------+---------------------+----------------+\n",
      "\n",
      "\n",
      "Features displaying the strongest predictive signal:\n",
      "+---------------------+--------------------+----------------+\n",
      "|feature              |pearson_correlation |feature_strength|\n",
      "+---------------------+--------------------+----------------+\n",
      "|membership_category  |-0.4644145540752387 |Strong          |\n",
      "|avg_transaction_value|-0.21670086929826687|Moderate        |\n",
      "|points_in_wallet     |-0.2077226314261469 |Moderate        |\n",
      "+---------------------+--------------------+----------------+\n",
      "\n",
      "\n",
      "===== Baseline Model: Assemble Features =====\n",
      "Assembling features...\n",
      "Combining all features into single vector...\n",
      "Splitting data...\n",
      "80.0/20.0 Test Train Split...\n",
      "\n",
      "===== Baseline Model: Create and Score =====\n",
      "Baseline model F1 Score - All Features: 0.93422\n",
      "\n",
      "===== Baseline Model: Feature Information Gain =====\n",
      "All Feature Importances by Information Gain:\n",
      "+----------------------------+------------------+-------------+\n",
      "|feature                     |gain              |gain_strength|\n",
      "+----------------------------+------------------+-------------+\n",
      "|points_in_wallet            |47.32978820800781 |Strong       |\n",
      "|membership_category         |46.96370315551758 |Strong       |\n",
      "|feedback                    |10.08957290649414 |Moderate     |\n",
      "|avg_transaction_value       |3.6815388202667236|Weak         |\n",
      "|medium_of_operation         |2.211885929107666 |Weak         |\n",
      "|past_complaint              |2.1779184341430664|Weak         |\n",
      "|join_month                  |2.1477320194244385|Weak         |\n",
      "|used_special_discount       |2.03133225440979  |Weak         |\n",
      "|join_year                   |2.022318124771118 |Weak         |\n",
      "|avg_time_spent              |2.010990619659424 |Weak         |\n",
      "|offer_application_preference|2.0051374435424805|Weak         |\n",
      "|avg_frequency_login_days    |1.9955650568008423|Weak         |\n",
      "|age                         |1.9770606756210327|Weak         |\n",
      "|last_visit_hour             |1.9758554697036743|Weak         |\n",
      "|last_visit_min              |1.9647328853607178|Weak         |\n",
      "|complaint_status            |1.9531728029251099|Weak         |\n",
      "|days_since_last_login       |1.9450739622116089|Weak         |\n",
      "|joined_through_referral     |1.9116102457046509|Weak         |\n",
      "|join_day                    |1.9077740907669067|Weak         |\n",
      "|last_visit_sec              |1.9041028022766113|Weak         |\n",
      "|preferred_offer_types       |1.8674798011779785|Weak         |\n",
      "|region_category             |1.8047860860824585|Weak         |\n",
      "|gender                      |1.69454026222229  |Weak         |\n",
      "|internet_option             |1.6462969779968262|Weak         |\n",
      "+----------------------------+------------------+-------------+\n",
      "\n",
      "Features with stronger information gain scores...\n",
      "+-------------------+-----------------+-------------+\n",
      "|feature            |gain             |gain_strength|\n",
      "+-------------------+-----------------+-------------+\n",
      "|feedback           |10.08957290649414|Moderate     |\n",
      "|points_in_wallet   |47.32978820800781|Strong       |\n",
      "|membership_category|46.96370315551758|Strong       |\n",
      "+-------------------+-----------------+-------------+\n",
      "\n",
      "\n",
      "***** TRAIN AND SCORE OPTIMISED MODEL *****\n",
      "\n",
      "===== Optimised Model: Strongest Features =====\n",
      "Strong feature list: ['avg_transaction_value', 'points_in_wallet', 'feedback', 'membership_category']\n",
      "\n",
      "===== Optimised Model: K-Fold Cross Validation =====\n",
      "Running 3-fold CV on training data using model: GBT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[strongFeatures: vector, churn_risk_score: int, rawPrediction: vector, probability: vector, prediction: double]\n",
      "F1 score on test data: 0.93301\n",
      "\n",
      "***** SELECT BEST MODEL AND PREDICT UNSEEN DATA *****\n",
      "\n",
      "===== Select Best Model =====\n",
      "Base model F1 Score: 0.93422\n",
      "Optimised model F1 Score: 0.93301\n",
      "Base model selected...\n",
      "\n",
      "===== Apply Model to Unseen Data =====\n",
      "\n",
      "----- Prediction Results (base model) -----\n",
      "+-----------------------+--------------------------+\n",
      "|actual_churn_risk_score|predicted_churn_risk_score|\n",
      "+-----------------------+--------------------------+\n",
      "|1                      |1.0                       |\n",
      "|0                      |0.0                       |\n",
      "|0                      |0.0                       |\n",
      "|1                      |1.0                       |\n",
      "|0                      |0.0                       |\n",
      "|1                      |1.0                       |\n",
      "|1                      |1.0                       |\n",
      "|1                      |1.0                       |\n",
      "|0                      |0.0                       |\n",
      "|1                      |1.0                       |\n",
      "|1                      |1.0                       |\n",
      "|0                      |0.0                       |\n",
      "|1                      |1.0                       |\n",
      "|1                      |1.0                       |\n",
      "|0                      |0.0                       |\n",
      "|0                      |0.0                       |\n",
      "|1                      |1.0                       |\n",
      "|1                      |1.0                       |\n",
      "|1                      |1.0                       |\n",
      "|1                      |1.0                       |\n",
      "+-----------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Prediction Accuracy: 93.27% (194/208)\n"
     ]
    }
   ],
   "source": [
    "execute_ml_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
