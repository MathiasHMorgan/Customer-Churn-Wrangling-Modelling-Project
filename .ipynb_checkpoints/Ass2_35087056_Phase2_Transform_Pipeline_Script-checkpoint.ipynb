{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c463458d",
   "metadata": {},
   "source": [
    "# Mathias Henry Morgan\n",
    "\n",
    "## 35087056\n",
    "\n",
    "## Assessment 2: Big data project - Phase 2\n",
    "\n",
    "## Data Cleaning and Wrangling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63f3791",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. ***Run all*** to import libraries, define function logic and trigger the execute_transform_pipeline() function\n",
    "2. The cell block containing execute_transform_pipeline() will contain logging statements for visibility of actions performed\n",
    "3. At the completion of the run a cleaned file will be generated with a unique date time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8270c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings('ignore') # Removes warning messages from print logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789fc09a",
   "metadata": {},
   "source": [
    "# Raw Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "695271ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_data_info(churnDataFrame: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Prints metadata of raw churn data set.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Print relevant meta data\n",
    "        print(f\"Column Count: {len(churnDataFrame.columns)}\")\n",
    "        print(f\"Row Count: {len(churnDataFrame)}\")\n",
    "        print(churnDataFrame.info())\n",
    "    except Exception as e:\n",
    "        print(f\"Error while printing metadata: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6616f",
   "metadata": {},
   "source": [
    "# Remove Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b22fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_errors(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes rows where 'avg_frequency_login_days' contains the string 'Error'.\n",
    "    :param churnDataFrame: DataFrame containing churn data.\n",
    "    :return: Processed DataFrame with error rows removed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rowCount = len(churnDataFrame)\n",
    "        errorCounts = len(churnDataFrame[churnDataFrame[\"avg_frequency_login_days\"] == \"Error\"])\n",
    "        churnDataFrame = churnDataFrame[churnDataFrame[\"avg_frequency_login_days\"] != \"Error\"]\n",
    "        print(f'{errorCounts} Errors Rows Removed out of {rowCount}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error while removing rows with 'Error' in 'avg_frequency_login_days': {e}\")\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb7c53",
   "metadata": {},
   "source": [
    "# Remove Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20702181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes rows with duplicate 'security_no' values, keeping the first occurrence.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn datas set with duplicate rows removed if present.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        originalSize = len(churnDataFrame)\n",
    "        churnDataFrame = churnDataFrame.drop_duplicates(subset=\"security_no\", keep=\"first\") # Remove duplicates if present, keep first record if duplicated\n",
    "        NewSize = len(churnDataFrame)\n",
    "        numDuplicates = originalSize - NewSize\n",
    "        print(f\"Number of duplicates removed: {numDuplicates}\") # Print number of duplicates present\n",
    "    except Exception as e:\n",
    "        print(f\"Error during duplicate removal: {e}\")\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adafe8dd",
   "metadata": {},
   "source": [
    "# Churn Risk Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ae289bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def churn_distrubtion(churnDataFrame: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Displays the percentage of customers at risk and not at risk of churn,\n",
    "    based on the 'churn_risk_score' column (if present).\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cols = churnDataFrame.columns\n",
    "        if \"churn_risk_score\" in cols: # Checks if 'churn_risk_score' is present\n",
    "            print((churnDataFrame[\"churn_risk_score\"].value_counts( # Print dataframe of churn risk distribution\n",
    "                normalize=True) * 100).round(2).to_frame().T.rename(columns={1:\"Percentage At Risk\", 0:\"Percentage Not At Risk\"}))\n",
    "            churnDataFrame[\"churn_risk_score\"] = churnDataFrame[\"churn_risk_score\"].astype(\"Int64\")\n",
    "        else:\n",
    "            print(\"'churn_risk_score' field not present in data set\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while calculating churn distribution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ab5c9",
   "metadata": {},
   "source": [
    "# Numeric Field Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92509174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_transform(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detects and handles extreme outliers in specified integer and float columns of the churn dataset\n",
    "    using the Interquartile Range (IQR) method. Far outliers are considered invalid records and are\n",
    "    replaced with the median of the respective column (excluding the outliers). All values are converted\n",
    "    to absolute, numeric types and roundedâ€”integers to whole numbers, floats to two decimal places.\n",
    "    :param churnDataFrame: DataFrame containing churn data.\n",
    "    :return: Processed DataFrame with erroneous outliers imputed with the median and rounded.\n",
    "    \"\"\"\n",
    "    intColsTransform = [\"days_since_last_login\", \"age\", \"avg_frequency_login_days\"]\n",
    "    floatColsTransform = [\"avg_time_spent\", \"avg_transaction_value\", \"points_in_wallet\"]\n",
    "\n",
    "    for col in intColsTransform:\n",
    "        try:\n",
    "            print(f\"Processing Integer values in '{col}'...\")\n",
    "            churnDataFrame[col] = abs(pd.to_numeric(churnDataFrame[col], errors=\"coerce\")).round(0).astype('Int64')\n",
    "            Q1 = churnDataFrame[col].quantile(0.10)\n",
    "            Q3 = churnDataFrame[col].quantile(0.90)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 4 * IQR\n",
    "            upper_bound = Q3 + 4 * IQR\n",
    "            intOutliers = churnDataFrame[col][(churnDataFrame[col] < lower_bound) | (churnDataFrame[col] > upper_bound)]\n",
    "            if intOutliers.empty:\n",
    "                print(f\"No far outliers found in {col}...\")\n",
    "            else:\n",
    "                print(f\"{len(intOutliers)} far outliers found in {col}: {sorted(set(intOutliers))}...\")\n",
    "                medianVal = churnDataFrame[col].loc[~churnDataFrame.index.isin(intOutliers.index)].median()\n",
    "                churnDataFrame.loc[churnDataFrame.index.isin(intOutliers.index), col] = medianVal\n",
    "                print(f\"The median value of {medianVal} imputed for all outliers\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing integer column '{col}': {e}\")\n",
    "\n",
    "    for col in floatColsTransform:\n",
    "        try:\n",
    "            print(f\"Processing float values in '{col}'...\")\n",
    "            churnDataFrame[col] = abs(pd.to_numeric(churnDataFrame[col], errors=\"coerce\")).round(2).astype(\"float64\")\n",
    "            Q1 = churnDataFrame[col].quantile(0.10)\n",
    "            Q3 = churnDataFrame[col].quantile(0.90)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 4 * IQR\n",
    "            upper_bound = Q3 + 4 * IQR\n",
    "            floatOutliers = churnDataFrame[col][(churnDataFrame[col] < lower_bound) | (churnDataFrame[col] > upper_bound)]\n",
    "            if floatOutliers.empty:\n",
    "                print(f\"No far outliers found in {col}...\")\n",
    "            else:\n",
    "                print(f\"{len(floatOutliers)} far outliers found in {col}: {sorted(set(floatOutliers))}...\")\n",
    "                medianVal = churnDataFrame[col].loc[~churnDataFrame.index.isin(floatOutliers.index)].median()\n",
    "                churnDataFrame.loc[churnDataFrame.index.isin(floatOutliers.index), col] = medianVal\n",
    "                print(f\"The median value of {medianVal} imputed for all outliers\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing float column '{col}': {e}\")\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445258b",
   "metadata": {},
   "source": [
    "# Process Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "024eaa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nulls(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identifies columns with null values and imputes missing data based on the column's data type.\n",
    "    String columns are imputed with 'Unknown', numeric columns with 0 (rounded to two decimals), and\n",
    "    boolean-like columns with 'No'. Provides visibility into null counts before and after processing.\n",
    "    :param churnDataFrame: DataFrame containing churn data.\n",
    "    :return: Processed DataFrame with null values imputed based on field type.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Identify columns containing null records...\")\n",
    "        missingDataCols = list(churnDataFrame.isna().sum()[churnDataFrame.isna().sum() > 0].to_frame().T.columns)\n",
    "        for col in missingDataCols:\n",
    "            print(f\"{col} contains: {churnDataFrame[col].isna().sum()} null values\")\n",
    "        boolCols = [\"used_special_discount\", \"offer_application_preference\", \"past_complaint\"]\n",
    "        stringCols = [col for col in list(churnDataFrame.select_dtypes(include=\"object\").columns) if col in missingDataCols and col not in boolCols]\n",
    "        numericCols = [col for col in list(churnDataFrame.select_dtypes(include=[\"int64\", \"float64\"]).columns) if col in missingDataCols]\n",
    "\n",
    "        print()\n",
    "        print(\"Processing null values in string columns...\")\n",
    "        for col in stringCols:\n",
    "            colNullCount = churnDataFrame[col].isna().sum()\n",
    "            print(f\"Processing {colNullCount} null records in {col}...\")\n",
    "            churnDataFrame[col] = churnDataFrame[col].apply(lambda row: \"Unknown\" if pd.isnull(row) else row)\n",
    "\n",
    "        print()\n",
    "        print(\"Processing null values in numeric columns...\")\n",
    "        for col in numericCols:\n",
    "            colNullCount = churnDataFrame[col].isna().sum()\n",
    "            print(f\"Processing {colNullCount} null records in {col}...\")\n",
    "            churnDataFrame[col] = churnDataFrame[col].apply(lambda row: 0 if pd.isnull(row) else row).round(2)\n",
    "\n",
    "        print()\n",
    "        print(\"Processing null values in boolean columns...\")\n",
    "        for col in boolCols:\n",
    "            if col in churnDataFrame.columns:\n",
    "                colNullCount = churnDataFrame[col].isna().sum()\n",
    "                print(f\"Processing {colNullCount} null records in {col}...\")\n",
    "                churnDataFrame[col] = churnDataFrame[col].apply(lambda row: 'No' if pd.isnull(row) else row)\n",
    "\n",
    "        print()\n",
    "        print(\"Null records processed...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing null values: {e}\")\n",
    "\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b5fbb",
   "metadata": {},
   "source": [
    "# Process Date and Time Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24138db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_transformation(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the 'joining_date' field to datetime format and splits it into separate\n",
    "    year, month, and day columns.\n",
    "    The original 'joining_date' column, which contains date values in the format\n",
    "    Year-Month-Day, is removed after transformation.\n",
    "    :param churnDataFrame: DataFrame containing churn data.\n",
    "    :return: Processed DataFrame with date fields split into 'join_year', 'join_month', and 'join_day'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        churnDataFrame[\"joining_date\"] = pd.to_datetime(churnDataFrame[\"joining_date\"])\n",
    "        churnDataFrame[\"join_year\"] = churnDataFrame[\"joining_date\"].dt.year.astype(\"Int64\")\n",
    "        churnDataFrame[\"join_month\"] = churnDataFrame[\"joining_date\"].dt.month.astype(\"Int64\")\n",
    "        churnDataFrame[\"join_day\"] = churnDataFrame[\"joining_date\"].dt.day.astype(\"Int64\")\n",
    "        churnDataFrame = churnDataFrame.drop(columns=[\"joining_date\"])\n",
    "        print(\"joining_date field split into: 'join_year', 'join_month', 'join_day'...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in date transformation: {e}\")\n",
    "    return churnDataFrame\n",
    "\n",
    "def time_transformation(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the 'last_visit_time' field to datetime format (HH:MM:SS) and splits it into\n",
    "    hour, minute, and second columns.\n",
    "    The original 'last_visit_time' column is removed after transformation.\n",
    "    :param churnDataFrame: DataFrame containing churn data.\n",
    "    :return: Processed DataFrame with time fields split into 'last_visit_hour', 'last_visit_min', and 'last_visit_sec'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        churnDataFrame[\"last_visit_time\"] = pd.to_datetime(churnDataFrame[\"last_visit_time\"], format=\"%H:%M:%S\")\n",
    "        churnDataFrame[\"last_visit_hour\"] = churnDataFrame[\"last_visit_time\"].dt.hour.astype(\"Int64\")\n",
    "        churnDataFrame[\"last_visit_min\"] = churnDataFrame[\"last_visit_time\"].dt.minute.astype(\"Int64\")\n",
    "        churnDataFrame[\"last_visit_sec\"] = churnDataFrame[\"last_visit_time\"].dt.second.astype(\"Int64\")\n",
    "        churnDataFrame = churnDataFrame.drop(columns=[\"last_visit_time\"])\n",
    "        print(\"last_visit_time field split into: 'last_visit_hour', 'last_visit_min', 'last_visit_sec'...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in time transformation: {e}\")\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12f3fc",
   "metadata": {},
   "source": [
    "# Process Special Character Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28c1adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_characters(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identifies and handles special characters (e.g., !, ?, #) in column values,\n",
    "    as they are considered erroneous. Replaces such values with appropriate substitutes.\n",
    "    The 'joined_through_referral' column is handled separately based on 'referral_id' logic.\n",
    "    :returns: processed churn data set with errenous special characters removed and values imputed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        specialChars = r\"[!?#]\" # Define special characters to find\n",
    "        colsSpecialChar = [\n",
    "            col for col in churnDataFrame.columns\n",
    "            if churnDataFrame[col].astype(str).str.contains(specialChars, regex=True).any()] # List comprehension of columns containing special character regex pattern\n",
    "        print(f\"Columns containing special character values: {colsSpecialChar}\")\n",
    "\n",
    "        # Loop over target columns\n",
    "        for col in colsSpecialChar:\n",
    "            uniqueVals = list(set(churnDataFrame[col].to_list())) # Get list of unique special characters in column\n",
    "            SpecialCharCount = churnDataFrame[col].str.count(specialChars).sum()\n",
    "            if col in \"joined_through_referral\": # Handle this field specifically, if ID is present can assume Yes else No\n",
    "                churnDataFrame[col] = churnDataFrame.apply(\n",
    "                    lambda row: \"No\" if row[\"referral_id\"] in [\"xxxxxxxx\"] and row[\"joined_through_referral\"] in uniqueVals else \"Yes\",\n",
    "                    axis=1) # Row level imputation\n",
    "            else:\n",
    "                churnDataFrame[col] = churnDataFrame[col].apply(lambda row: \"Unknown\" if row in specialChars else row) # Otherwise impute calue at row level\n",
    "            print(f\"Processed {SpecialCharCount} records of special charcters in '{col}' column.....\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in special character handling: {e}\")\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c120274",
   "metadata": {},
   "source": [
    "# Sentiment Mapping of Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ccc9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_transform(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms the 'feedback' column into sentiment classifications using a predefined mapping:\n",
    "    0 = Negative, 1 = Positive, 2 = Neutral.\n",
    "    This transformation uses hardcoded mappings based on known feedback values.\n",
    "    Trailing spaces are stripped before applying the mapping to ensure consistent classification.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn data set with feedback values mapped to pre defined integers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Transforming feedback values into sentiment classification\n",
    "        sentimentMapping = {\n",
    "            \"Products always in Stock\":1,\n",
    "            \"User Friendly Website\":1,\n",
    "            \"Poor Customer Service\":0,\n",
    "            \"Poor Product Quality\":0,\n",
    "            \"Reasonable Price\":1,\n",
    "            \"Quality Customer Care\":1,\n",
    "            \"Too many ads\":0,\n",
    "            \"Poor Website\":0,\n",
    "            \"No reason specified\":2}\n",
    "        # Loop over mappings dictionary and display\n",
    "        for k,v in sentimentMapping.items():\n",
    "            print(f\"Mapping value: '{k}', to integer value: {v}\")\n",
    "        churnDataFrame[\"feedback\"] = churnDataFrame[\"feedback\"].str.strip() # Ensure no trailing spaces are present in the field to allow for proper value conversion\n",
    "        churnDataFrame[\"feedback\"] = churnDataFrame[\"feedback\"].apply(lambda row: sentimentMapping[row] if row in sentimentMapping else row) # Alter values using row level imputation on field\n",
    "        churnDataFrame[\"feedback\"] = churnDataFrame[\"feedback\"].astype('Int64')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in feedback transformation: {e}\")\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919028e",
   "metadata": {},
   "source": [
    "# Label Encode Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27ca5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs label encoding on all categorical columns, converting string values\n",
    "    into integers for compatibility with tree-based models. The value mappings for each column\n",
    "    are displayed in the logs.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn data set with categorical variables conerted to integers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Identify categorical fields and loop over them to convert categorical values\n",
    "        categoricalCols = list(churnDataFrame.select_dtypes(include='object').columns)\n",
    "        for col in categoricalCols:\n",
    "            print(f\"Label Encoding values in {col}...\")\n",
    "            categoricalVals = list(set(churnDataFrame[col])) # Get a list of all necessary columns\n",
    "            labelEncoder = preprocessing.LabelEncoder() # Define Label Encoder from\n",
    "            churnDataFrame[col] = labelEncoder.fit_transform(churnDataFrame[col]) # Apply the encoding to target field\n",
    "            churnDataFrame[col] = churnDataFrame[col].astype(\"Int64\")\n",
    "            print(f\"Categorical values mapped from {categoricalVals} to integer values: {list(set(churnDataFrame[col]))}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in label encoding: {e}\")\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776be703",
   "metadata": {},
   "source": [
    "# Remove unwanted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9df20aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(churnDataFrame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes the 'security_no' and 'referral_id' columns from the dataset and\n",
    "    reorders the 'churn_risk_score' column to appear last, if present.\n",
    "    :param churnDataFrame: churn data set.\n",
    "    :returns: processed churn data set with certain fields removed and columns re-ordered.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove unnecessary fields\n",
    "        churnDataFrame = churnDataFrame.drop(columns=[\"security_no\",\"referral_id\"])\n",
    "        print(\"Removed 'security_no' and 'referral_id' columns...\")\n",
    "        cols = [col for col in churnDataFrame.columns if col not in [\"churn_risk_score\"]] # Get a list of call fields without churn_risk_score\n",
    "        if \"churn_risk_score\" in churnDataFrame.columns:\n",
    "            newCols = cols + [\"churn_risk_score\"] # Add churn_risk_score to the end if present\n",
    "            churnDataFrame = churnDataFrame.reindex(columns=newCols) # Alter column order ensuring churn_risk_score is the last field in data frame reading left to right\n",
    "            print(\"Re-Indexed 'churn_risk_score' field\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature removal: {e}\")\n",
    "    return churnDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b0a8a",
   "metadata": {},
   "source": [
    "# Execute Pipeline Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "042b8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_transform_pipeline() -> None:\n",
    "    \"\"\"\n",
    "    Executes the full data cleaning and transformation pipeline for churn data.\n",
    "    This function processes all raw churn CSV files in the current directory that match\n",
    "    the pattern 'churn_raw*.csv'. It sequentially applies a series of transformation\n",
    "    steps, including error removal, deduplication, null imputation, encoding, and more.\n",
    "    For each file, a unique run ID is generated and logged. The cleaned dataset is\n",
    "    exported as a new CSV file with a timestamped filename.\n",
    "    :returns: None\n",
    "    \"\"\"\n",
    "    # Extract the raw data csv file to be processed\n",
    "    csvFiles = [str(file) for file in Path(\".\").glob(\"churn_raw*.csv\")] # Get a list of all CSV files with the name churn_raw.csv OR and extension e.g churn_raw_1.csv\n",
    "    for fileName in csvFiles:\n",
    "        try:\n",
    "            churnData = pd.read_csv(fileName)\n",
    "\n",
    "            # Creating random ID to simulate a real data transformation pipeline in log output\n",
    "            randomId = uuid.uuid4()\n",
    "            stringID = str(randomId)\n",
    "            print(f\"Executing Data Transformation Pipeline...\")\n",
    "            print() # Creates a gap for for clean logging output\n",
    "            print(f\"Run ID: {stringID}\")\n",
    "            print()\n",
    "\n",
    "            print(\"----- Raw Churn Metadata -----\",end=\"\\n\")\n",
    "            meta_data_info(churnData)\n",
    "            print()\n",
    "\n",
    "            print(\"----- Removing Errenous Rows -----\",end=\"\\n\")\n",
    "            churnData = remove_errors(churnData)\n",
    "            print()\n",
    "\n",
    "            print(\"----- Removing Duplicate Rows -----\",end=\"\\n\")\n",
    "            churnData = remove_duplicates(churnData)\n",
    "            print()\n",
    "\n",
    "            print(\"----- Distribution (%) of Customers at risk of Churn -----\",end=\"\\n\")\n",
    "            churn_distrubtion(churnData)\n",
    "            print()\n",
    "\n",
    "            print(\"----- Transform Numeric Values -----\",end=\"\\n\")\n",
    "            churnData = numeric_transform(churnData)\n",
    "            print()\n",
    "\n",
    "            print(\"----- Impute Values for Null Records -----\",end=\"\\n\")\n",
    "            churnData = process_nulls(churnData)\n",
    "            print()\n",
    "\n",
    "            print(\"----- Transform Date and Time Fields -----\",end=\"\\n\")\n",
    "            churnData = date_transformation(churnData)\n",
    "            churnData = time_transformation(churnData)\n",
    "            print()\n",
    "\n",
    "            print(\"----- Impute Values for Special Characters -----\",end=\"\\n\")\n",
    "            churnData = special_characters(churnData)\n",
    "            print()\n",
    "\n",
    "            print(\"----- Transform Feedback Column -----\",end=\"\\n\")\n",
    "            churnData = feedback_transform(churnData)\n",
    "            print()\n",
    "\n",
    "            print(\"----- Removing Unnecessary Features -----\",end=\"\\n\")\n",
    "            churnData = remove_features(churnData)\n",
    "            print()\n",
    "\n",
    "            print(\"----- Encode Categorical Variables -----\",end=\"\\n\")\n",
    "            churnData = label_encode(churnData)\n",
    "            print()\n",
    "\n",
    "            print(\"----- Processed Churn Metadata -----\",end=\"\\n\")\n",
    "            if len(churnData) > 20000:\n",
    "                churnData = churnData.iloc[:20000] # Limit row count to 20,000 rows\n",
    "            meta_data_info(churnData)\n",
    "            print()\n",
    "\n",
    "            print(f\"Data Transformation Run ID: {stringID} Complete...\")\n",
    "\n",
    "            currentTime = datetime.datetime.now() # Get current date and time\n",
    "            fileName = f\"churn_clean_{currentTime}.csv\" # Define cleaned file name with unique date time stamp\n",
    "            cleanFolderPath = f\"{fileName}\"\n",
    "            churnData.to_csv(cleanFolderPath, index=False) # Write out cleaned CSV file to folder\n",
    "            print(f\"Transformed data set: {fileName} written to main folder...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing raw file '{fileName}'': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b61f68",
   "metadata": {},
   "source": [
    "# Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c92871bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Data Transformation Pipeline...\n",
      "\n",
      "Run ID: a40cfddc-d551-4ddb-a799-d05bd8e4cd85\n",
      "\n",
      "----- Raw Churn Metadata -----\n",
      "Column Count: 23\n",
      "Row Count: 36992\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36992 entries, 0 to 36991\n",
      "Data columns (total 23 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   age                           36992 non-null  int64  \n",
      " 1   gender                        36992 non-null  object \n",
      " 2   security_no                   36992 non-null  object \n",
      " 3   region_category               31564 non-null  object \n",
      " 4   membership_category           36992 non-null  object \n",
      " 5   joining_date                  36992 non-null  object \n",
      " 6   joined_through_referral       36992 non-null  object \n",
      " 7   referral_id                   36992 non-null  object \n",
      " 8   preferred_offer_types         36704 non-null  object \n",
      " 9   medium_of_operation           36992 non-null  object \n",
      " 10  internet_option               36992 non-null  object \n",
      " 11  last_visit_time               36992 non-null  object \n",
      " 12  days_since_last_login         36992 non-null  int64  \n",
      " 13  avg_time_spent                36992 non-null  float64\n",
      " 14  avg_transaction_value         36992 non-null  float64\n",
      " 15  avg_frequency_login_days      36992 non-null  object \n",
      " 16  points_in_wallet              33549 non-null  float64\n",
      " 17  used_special_discount         36992 non-null  object \n",
      " 18  offer_application_preference  36992 non-null  object \n",
      " 19  past_complaint                36992 non-null  object \n",
      " 20  complaint_status              36992 non-null  object \n",
      " 21  feedback                      36992 non-null  object \n",
      " 22  churn_risk_score              36992 non-null  int64  \n",
      "dtypes: float64(3), int64(3), object(17)\n",
      "memory usage: 6.5+ MB\n",
      "None\n",
      "\n",
      "----- Removing Errenous Rows -----\n",
      "3522 Errors Rows Removed out of 36992\n",
      "\n",
      "----- Removing Duplicate Rows -----\n",
      "Number of duplicates removed: 0\n",
      "\n",
      "----- Distribution (%) of Customers at risk of Churn -----\n",
      "churn_risk_score  Percentage At Risk  Percentage Not At Risk\n",
      "proportion                     54.07                   45.93\n",
      "\n",
      "----- Transform Numeric Values -----\n",
      "Processing Integer values in 'days_since_last_login'...\n",
      "1817 far outliers found in days_since_last_login: [999]...\n",
      "The median value of 13.0 imputed for all outliers\n",
      "Processing Integer values in 'age'...\n",
      "No far outliers found in age...\n",
      "Processing Integer values in 'avg_frequency_login_days'...\n",
      "No far outliers found in avg_frequency_login_days...\n",
      "Processing float values in 'avg_time_spent'...\n",
      "No far outliers found in avg_time_spent...\n",
      "Processing float values in 'avg_transaction_value'...\n",
      "No far outliers found in avg_transaction_value...\n",
      "Processing float values in 'points_in_wallet'...\n",
      "No far outliers found in points_in_wallet...\n",
      "\n",
      "----- Impute Values for Null Records -----\n",
      "Identify columns containing null records...\n",
      "region_category contains: 4906 null values\n",
      "preferred_offer_types contains: 262 null values\n",
      "points_in_wallet contains: 3118 null values\n",
      "\n",
      "Processing null values in string columns...\n",
      "Processing 4906 null records in region_category...\n",
      "Processing 262 null records in preferred_offer_types...\n",
      "\n",
      "Processing null values in numeric columns...\n",
      "Processing 3118 null records in points_in_wallet...\n",
      "\n",
      "Processing null values in boolean columns...\n",
      "Processing 0 null records in used_special_discount...\n",
      "Processing 0 null records in offer_application_preference...\n",
      "Processing 0 null records in past_complaint...\n",
      "\n",
      "Null records processed...\n",
      "\n",
      "----- Transform Date and Time Fields -----\n",
      "joining_date field split into: 'join_year', 'join_month', 'join_day'...\n",
      "last_visit_time field split into: 'last_visit_hour', 'last_visit_min', 'last_visit_sec'...\n",
      "\n",
      "----- Impute Values for Special Characters -----\n",
      "Columns containing special character values: ['joined_through_referral', 'medium_of_operation']\n",
      "Processed 4924 records of special charcters in 'joined_through_referral' column.....\n",
      "Processed 4859 records of special charcters in 'medium_of_operation' column.....\n",
      "\n",
      "----- Transform Feedback Column -----\n",
      "Mapping value: 'Products always in Stock', to integer value: 1\n",
      "Mapping value: 'User Friendly Website', to integer value: 1\n",
      "Mapping value: 'Poor Customer Service', to integer value: 0\n",
      "Mapping value: 'Poor Product Quality', to integer value: 0\n",
      "Mapping value: 'Reasonable Price', to integer value: 1\n",
      "Mapping value: 'Quality Customer Care', to integer value: 1\n",
      "Mapping value: 'Too many ads', to integer value: 0\n",
      "Mapping value: 'Poor Website', to integer value: 0\n",
      "Mapping value: 'No reason specified', to integer value: 2\n",
      "\n",
      "----- Removing Unnecessary Features -----\n",
      "Removed 'security_no' and 'referral_id' columns...\n",
      "Re-Indexed 'churn_risk_score' field\n",
      "\n",
      "----- Encode Categorical Variables -----\n",
      "Label Encoding values in gender...\n",
      "Categorical values mapped from ['Unknown', 'M', 'F'] to integer values: [0, 1, 2]\n",
      "Label Encoding values in region_category...\n",
      "Categorical values mapped from ['Town', 'Unknown', 'Village', 'City'] to integer values: [0, 1, 2, 3]\n",
      "Label Encoding values in membership_category...\n",
      "Categorical values mapped from ['Premium Membership', 'Silver Membership', 'Gold Membership', 'Basic Membership', 'No Membership', 'Platinum Membership'] to integer values: [0, 1, 2, 3, 4, 5]\n",
      "Label Encoding values in joined_through_referral...\n",
      "Categorical values mapped from ['No', 'Yes'] to integer values: [0, 1]\n",
      "Label Encoding values in preferred_offer_types...\n",
      "Categorical values mapped from ['Gift Vouchers/Coupons', 'Credit/Debit Card Offers', 'Unknown', 'Without Offers'] to integer values: [0, 1, 2, 3]\n",
      "Label Encoding values in medium_of_operation...\n",
      "Categorical values mapped from ['Desktop', 'Unknown', 'Both', 'Smartphone'] to integer values: [0, 1, 2, 3]\n",
      "Label Encoding values in internet_option...\n",
      "Categorical values mapped from ['Wi-Fi', 'Mobile_Data', 'Fiber_Optic'] to integer values: [0, 1, 2]\n",
      "Label Encoding values in used_special_discount...\n",
      "Categorical values mapped from ['No', 'Yes'] to integer values: [0, 1]\n",
      "Label Encoding values in offer_application_preference...\n",
      "Categorical values mapped from ['No', 'Yes'] to integer values: [0, 1]\n",
      "Label Encoding values in past_complaint...\n",
      "Categorical values mapped from ['No', 'Yes'] to integer values: [0, 1]\n",
      "Label Encoding values in complaint_status...\n",
      "Categorical values mapped from ['No Information Available', 'Solved in Follow-up', 'Unsolved', 'Solved', 'Not Applicable'] to integer values: [0, 1, 2, 3, 4]\n",
      "\n",
      "----- Processed Churn Metadata -----\n",
      "Column Count: 25\n",
      "Row Count: 20000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20000 entries, 0 to 22080\n",
      "Data columns (total 25 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   age                           20000 non-null  Int64  \n",
      " 1   gender                        20000 non-null  Int64  \n",
      " 2   region_category               20000 non-null  Int64  \n",
      " 3   membership_category           20000 non-null  Int64  \n",
      " 4   joined_through_referral       20000 non-null  Int64  \n",
      " 5   preferred_offer_types         20000 non-null  Int64  \n",
      " 6   medium_of_operation           20000 non-null  Int64  \n",
      " 7   internet_option               20000 non-null  Int64  \n",
      " 8   days_since_last_login         20000 non-null  Int64  \n",
      " 9   avg_time_spent                20000 non-null  float64\n",
      " 10  avg_transaction_value         20000 non-null  float64\n",
      " 11  avg_frequency_login_days      20000 non-null  Int64  \n",
      " 12  points_in_wallet              20000 non-null  float64\n",
      " 13  used_special_discount         20000 non-null  Int64  \n",
      " 14  offer_application_preference  20000 non-null  Int64  \n",
      " 15  past_complaint                20000 non-null  Int64  \n",
      " 16  complaint_status              20000 non-null  Int64  \n",
      " 17  feedback                      20000 non-null  Int64  \n",
      " 18  join_year                     20000 non-null  Int64  \n",
      " 19  join_month                    20000 non-null  Int64  \n",
      " 20  join_day                      20000 non-null  Int64  \n",
      " 21  last_visit_hour               20000 non-null  Int64  \n",
      " 22  last_visit_min                20000 non-null  Int64  \n",
      " 23  last_visit_sec                20000 non-null  Int64  \n",
      " 24  churn_risk_score              20000 non-null  Int64  \n",
      "dtypes: Int64(22), float64(3)\n",
      "memory usage: 4.4 MB\n",
      "None\n",
      "\n",
      "Data Transformation Run ID: a40cfddc-d551-4ddb-a799-d05bd8e4cd85 Complete...\n",
      "Transformed data set: churn_clean_2025-06-02 11:11:25.848224.csv written to main folder...\n"
     ]
    }
   ],
   "source": [
    "# Execute transformation pipeline\n",
    "execute_transform_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c242e0c",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "GeeksforGeeks. (n.d.). Change the order of a Pandas DataFrame columns in Python. https://www.geeksforgeeks.org/change-the-order-of-a-pandas-dataframe-columns-in-python/\n",
    "\n",
    "GeeksforGeeks. (2025, January 2). Detect and remove the outliers using Python. https://www.geeksforgeeks.org/detect-and-remove-the-outliers-using-python/\n",
    "\n",
    "GeeksforGeeks. (n.d.). Generating random IDs using UUID in Python. https://www.geeksforgeeks.org/generating-random-ids-using-uuid-python/\n",
    "\n",
    "GeeksforGeeks. (n.d.). Get current timestamp using Python. https://www.geeksforgeeks.org/get-current-timestamp-using-python/\n",
    "\n",
    "GeeksforGeeks. (2025, February 12). Label encoding in Python. https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python/\n",
    "\n",
    "nkmk. (n.d.). pandas: Select columns by dtype with select_dtypes(). nkmk note. https://note.nkmk.me/en/python-pandas-select-dtypes/\n",
    "\n",
    "Stack Overflow. (n.d.). Change column type in pandas. Stack Exchange. https://stackoverflow.com/questions/15891038/change-column-type-in-pandas\n",
    "\n",
    "Stack Overflow. (n.d.). Hide all warnings in IPython. Stack Exchange. https://stackoverflow.com/questions/9031783/hide-all-warnings-in-ipython\n",
    "\n",
    "Stack Overflow. (n.d.). Splitting timestamp column into separate date and time columns. Stack Exchange. https://stackoverflow.com/questions/35595710/splitting-timestamp-column-into-separate-date-and-time-columns\n",
    "\n",
    "Stack Overflow. (n.d.). Using a dictionary of lambda functions in pandas.assign() gives the wrong result. How to avoid the lazy binding? Stack Exchange. https://stackoverflow.com/questions/72544610/using-a-dictionary-of-lambda-functions-in-pandas-assign-gives-the-wrong-result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
